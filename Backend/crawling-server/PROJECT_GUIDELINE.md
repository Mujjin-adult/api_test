# ğŸ“˜ College Notice Crawler - í”„ë¡œì íŠ¸ ê°€ì´ë“œë¼ì¸

> ëŒ€í•™êµ ê³µì§€ì‚¬í•­ ìë™ ìˆ˜ì§‘ ì‹œìŠ¤í…œì˜ í¬ê´„ì ì¸ ê¸°ìˆ  ë¬¸ì„œ

**ë²„ì „**: 1.0
**ìµœì¢… ì—…ë°ì´íŠ¸**: 2025-11-03
**ì‘ì„±ì**: Development Team

---

## ğŸ“‘ ëª©ì°¨

1. [í”„ë¡œì íŠ¸ ê°œìš”](#1-í”„ë¡œì íŠ¸-ê°œìš”)
2. [ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜](#2-ì‹œìŠ¤í…œ-ì•„í‚¤í…ì²˜)
3. [ê¸°ìˆ  ìŠ¤íƒ](#3-ê¸°ìˆ -ìŠ¤íƒ)
4. [ë””ë ‰í† ë¦¬ êµ¬ì¡°](#4-ë””ë ‰í† ë¦¬-êµ¬ì¡°)
5. [í•µì‹¬ ì»´í¬ë„ŒíŠ¸](#5-í•µì‹¬-ì»´í¬ë„ŒíŠ¸)
6. [ë°ì´í„° íë¦„](#6-ë°ì´í„°-íë¦„)
7. [ì„¤ì • ë° í™˜ê²½](#7-ì„¤ì •-ë°-í™˜ê²½)
8. [í…ŒìŠ¤íŠ¸ ì „ëµ](#8-í…ŒìŠ¤íŠ¸-ì „ëµ)
9. [ë°°í¬ ë° ìš´ì˜](#9-ë°°í¬-ë°-ìš´ì˜)
10. [ê°œë°œ ê°€ì´ë“œ](#10-ê°œë°œ-ê°€ì´ë“œ)

---

## 1. í”„ë¡œì íŠ¸ ê°œìš”

### 1.1 í”„ë¡œì íŠ¸ ëª©ì 

**College Notice Crawler**ëŠ” ì—¬ëŸ¬ ëŒ€í•™êµ ì›¹ì‚¬ì´íŠ¸ì—ì„œ ê³µì§€ì‚¬í•­ì„ ìë™ìœ¼ë¡œ ìˆ˜ì§‘í•˜ê³  ê´€ë¦¬í•˜ëŠ” ë¶„ì‚° í¬ë¡¤ë§ ì‹œìŠ¤í…œì…ë‹ˆë‹¤. í•™ìƒë“¤ì´ ì—¬ëŸ¬ ì›¹ì‚¬ì´íŠ¸ë¥¼ ì¼ì¼ì´ í™•ì¸í•˜ì§€ ì•Šê³ ë„ ì¤‘ìš”í•œ ê³µì§€ì‚¬í•­ì„ ë†“ì¹˜ì§€ ì•Šë„ë¡ ë•ëŠ” ê²ƒì´ ëª©í‘œì…ë‹ˆë‹¤.

### 1.2 í•µì‹¬ ê¸°ëŠ¥

#### í¬ë¡¤ë§ ê¸°ëŠ¥
- ğŸ•·ï¸ **ë‹¤ì¤‘ ëŒ€í•™ ì§€ì›**: ì—¬ëŸ¬ ëŒ€í•™ ì›¹ì‚¬ì´íŠ¸ ë™ì‹œ í¬ë¡¤ë§
- ğŸ”„ **ìë™ ìŠ¤ì¼€ì¤„ë§**: Celery Beatë¥¼ í†µí•œ ì£¼ê¸°ì  ì‹¤í–‰
- ğŸ­ **ë™ì  í˜ì´ì§€ ì§€ì›**: Playwright ê¸°ë°˜ JavaScript ë Œë”ë§
- âš¡ **ë¶„ì‚° ì²˜ë¦¬**: Celery ì›Œì»¤ë¥¼ í†µí•œ ë³‘ë ¬ í¬ë¡¤ë§
- ğŸ” **ìŠ¤ë§ˆíŠ¸ ì¬ì‹œë„**: ì§€ìˆ˜ ë°±ì˜¤í”„ + Jitter ì•Œê³ ë¦¬ì¦˜

#### ë°ì´í„° ê´€ë¦¬
- ğŸ’¾ **êµ¬ì¡°í™”ëœ ì €ì¥**: PostgreSQL ê¸°ë°˜ ë°ì´í„° ê´€ë¦¬
- ğŸ” **ì¤‘ë³µ ì œê±°**: URL ë° content hash ê¸°ë°˜ í•„í„°ë§
- ğŸ“Š **í†µê³„ ì¶”ì **: í¬ë¡¤ë§ ì„±ê³µë¥  ë° ì‘ì—… í˜„í™© ëª¨ë‹ˆí„°ë§
- ğŸ“ **ë²„ì „ ê´€ë¦¬**: ë™ì¼ ê³µì§€ì‚¬í•­ì˜ ìŠ¤ëƒ…ìƒ· ê´€ë¦¬

#### API ë° ë³´ì•ˆ
- ğŸš€ **RESTful API**: FastAPI ê¸°ë°˜ ê³ ì„±ëŠ¥ ë¹„ë™ê¸° API
- ğŸ” **API Key ì¸ì¦**: ì•ˆì „í•œ ì ‘ê·¼ ì œì–´
- ğŸ›¡ï¸ **ë³´ì•ˆ ê°•í™”**: Rate Limiting, CORS, Circuit Breaker
- ğŸ“ˆ **ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§**: Prometheus + Grafana + Sentry

### 1.3 ì£¼ìš” ì‚¬ìš©ì ì‹œë‚˜ë¦¬ì˜¤

#### ì‹œë‚˜ë¦¬ì˜¤ 1: ì •ê¸° í¬ë¡¤ë§ ì„¤ì •
```
ê´€ë¦¬ì â†’ API í˜¸ì¶œ â†’ Job ìƒì„± (ìŠ¤ì¼€ì¤„ ì„¤ì •)
       â†’ Celery Beat â†’ ì£¼ê¸°ì  í¬ë¡¤ë§ ì‹¤í–‰
       â†’ ê²°ê³¼ ì €ì¥ â†’ ì›¹í›… ì•Œë¦¼
```

#### ì‹œë‚˜ë¦¬ì˜¤ 2: ìˆ˜ë™ í¬ë¡¤ë§ ì‹¤í–‰
```
ì‚¬ìš©ì â†’ Dashboard â†’ "í¬ë¡¤ë§ ì‹¤í–‰" ë²„íŠ¼
      â†’ API í˜¸ì¶œ â†’ Celery Task ìƒì„±
      â†’ ì‹¤ì‹œê°„ ìƒíƒœ í™•ì¸
```

#### ì‹œë‚˜ë¦¬ì˜¤ 3: ê³µì§€ì‚¬í•­ ì¡°íšŒ
```
ì•±/ì›¹ â†’ API ìš”ì²­ â†’ í•„í„°ë§ (ì¹´í…Œê³ ë¦¬, í‚¤ì›Œë“œ)
    â†’ PostgreSQL ì¿¼ë¦¬ â†’ JSON ì‘ë‹µ
```

---

## 2. ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜

### 2.1 ì „ì²´ ì‹œìŠ¤í…œ êµ¬ì¡°

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                          Client Layer                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚  â”‚Web Clientâ”‚  â”‚Mobile Appâ”‚  â”‚  cURL    â”‚  â”‚ Postman  â”‚       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜       â”‚
â”‚        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚
â”‚                           â”‚ HTTP/HTTPS                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   Presentation Layer                              â”‚
â”‚                           â”‚                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚
â”‚  â”‚           FastAPI Application                    â”‚            â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚            â”‚
â”‚  â”‚  â”‚  Middlewares                             â”‚  â”‚            â”‚
â”‚  â”‚  â”‚  - Security (API Key, CORS, Trusted)    â”‚  â”‚            â”‚
â”‚  â”‚  â”‚  - Rate Limiting (Redis-based)           â”‚  â”‚            â”‚
â”‚  â”‚  â”‚  - Metrics (Prometheus)                  â”‚  â”‚            â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚            â”‚
â”‚  â”‚                                                  â”‚            â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚            â”‚
â”‚  â”‚  â”‚  API Routers                             â”‚  â”‚            â”‚
â”‚  â”‚  â”‚  - /jobs      (Job ê´€ë¦¬)                â”‚  â”‚            â”‚
â”‚  â”‚  â”‚  - /tasks     (Task ì¡°íšŒ)               â”‚  â”‚            â”‚
â”‚  â”‚  â”‚  - /documents (ê³µì§€ì‚¬í•­ ì¡°íšŒ)           â”‚  â”‚            â”‚
â”‚  â”‚  â”‚  - /health    (í—¬ìŠ¤ ì²´í¬)               â”‚  â”‚            â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚            â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   Business Logic Layer                            â”‚
â”‚                           â”‚                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚
â”‚  â”‚  CRUD Operations (crud.py)                      â”‚            â”‚
â”‚  â”‚  - Job CRUD                                      â”‚            â”‚
â”‚  â”‚  - Task CRUD                                     â”‚            â”‚
â”‚  â”‚  - Document CRUD                                 â”‚            â”‚
â”‚  â”‚  - Statistics                                    â”‚            â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚
â”‚                            â”‚                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚
â”‚  â”‚  College Crawlers (college_crawlers.py)        â”‚            â”‚
â”‚  â”‚  - BaseCrawler (ì¶”ìƒ í´ë˜ìŠ¤)                   â”‚            â”‚
â”‚  â”‚  - KonkukCrawler                                 â”‚            â”‚
â”‚  â”‚  - SeoulTechCrawler                              â”‚            â”‚
â”‚  â”‚  - ... (í™•ì¥ ê°€ëŠ¥)                              â”‚            â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚
â”‚                            â”‚                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚
â”‚  â”‚  Resilience Patterns                            â”‚            â”‚
â”‚  â”‚  - Circuit Breaker (circuit_breaker.py)        â”‚            â”‚
â”‚  â”‚  - Rate Limiter (rate_limiter.py)              â”‚            â”‚
â”‚  â”‚  - Retry Logic (exponential backoff)           â”‚            â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Data Access Layer                              â”‚
â”‚                           â”‚                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚
â”‚  â”‚  SQLAlchemy ORM (models.py, database.py)       â”‚            â”‚
â”‚  â”‚  - Session Management                           â”‚            â”‚
â”‚  â”‚  - Connection Pool                               â”‚            â”‚
â”‚  â”‚  - Transaction Management                        â”‚            â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      Data Layer                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚
â”‚  â”‚         PostgreSQL Database                      â”‚            â”‚
â”‚  â”‚  Tables:                                         â”‚            â”‚
â”‚  â”‚  - crawl_job     (í¬ë¡¤ë§ ì‘ì—… ì •ì˜)            â”‚            â”‚
â”‚  â”‚  - crawl_task    (ê°œë³„ í¬ë¡¤ë§ íƒœìŠ¤í¬)          â”‚            â”‚
â”‚  â”‚  - crawl_notice  (ìˆ˜ì§‘ëœ ê³µì§€ì‚¬í•­)             â”‚            â”‚
â”‚  â”‚  - webhook       (ì´ë²¤íŠ¸ ì•Œë¦¼)                 â”‚            â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Task Queue Layer                                â”‚
â”‚                                                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚
â”‚  â”‚  Celery Beat   â”‚â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚  Redis Broker   â”‚                  â”‚
â”‚  â”‚  (Scheduler)   â”‚         â”‚  (Message Queue)â”‚                  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚
â”‚                                       â”‚                            â”‚
â”‚                             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”‚
â”‚                             â”‚  Celery Workers   â”‚                 â”‚
â”‚                             â”‚  (tasks.py)       â”‚                 â”‚
â”‚                             â”‚  - crawl_task     â”‚                 â”‚
â”‚                             â”‚  - process_task   â”‚                 â”‚
â”‚                             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  Monitoring & Observability                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚
â”‚  â”‚ Prometheus â”‚  â”‚  Grafana   â”‚  â”‚   Sentry   â”‚                  â”‚
â”‚  â”‚ (Metrics)  â”‚  â”‚(Dashboard) â”‚  â”‚  (Errors)  â”‚                  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2.2 ì»´í¬ë„ŒíŠ¸ ê°„ ìƒí˜¸ì‘ìš©

#### í¬ë¡¤ë§ ì‘ì—… ì‹¤í–‰ íë¦„
```
1. API Request (POST /jobs)
   â”‚
   â–¼
2. FastAPI Router (main.py)
   â”‚
   â–¼
3. Validation (Pydantic Schema)
   â”‚
   â–¼
4. CRUD Operation (crud.create_job)
   â”‚
   â–¼
5. Database (INSERT into crawl_job)
   â”‚
   â–¼
6. Celery Task Trigger (tasks.crawl_task.delay)
   â”‚
   â–¼
7. Redis Queue (Celery Broker)
   â”‚
   â–¼
8. Celery Worker Picks Up Task
   â”‚
   â–¼
9. Execute Crawler (college_crawlers.py)
   â”‚
   â–¼
10. Fetch Website (Playwright/Requests)
   â”‚
   â–¼
11. Parse HTML (BeautifulSoup)
   â”‚
   â–¼
12. Save Results (CRUD â†’ PostgreSQL)
   â”‚
   â–¼
13. Update Task Status
   â”‚
   â–¼
14. Send Webhook (Optional)
```

---

## 3. ê¸°ìˆ  ìŠ¤íƒ

### 3.1 Backend Framework
| ê¸°ìˆ  | ë²„ì „ | ìš©ë„ | ì„ íƒ ì´ìœ  |
|------|------|------|-----------|
| **FastAPI** | 0.104.1 | ì›¹ í”„ë ˆì„ì›Œí¬ | ê³ ì„±ëŠ¥ ë¹„ë™ê¸°, ìë™ API ë¬¸ì„œí™”, Type hints ì§€ì› |
| **Uvicorn** | 0.24.0 | ASGI ì„œë²„ | FastAPIì™€ ìµœì  í˜¸í™˜, ë†’ì€ ì²˜ë¦¬ëŸ‰ |
| **Pydantic** | 2.5.0 | ë°ì´í„° ê²€ì¦ | ê°•ë ¥í•œ íƒ€ì… ê²€ì¦, ì„¤ì • ê´€ë¦¬ |

### 3.2 Database
| ê¸°ìˆ  | ë²„ì „ | ìš©ë„ | ì„ íƒ ì´ìœ  |
|------|------|------|-----------|
| **PostgreSQL** | 15+ | ê´€ê³„í˜• DB | ACID ë³´ì¥, JSON ì§€ì›, í™•ì¥ì„± |
| **SQLAlchemy** | 2.0.23 | ORM | Python í‘œì¤€, ê°•ë ¥í•œ ì¿¼ë¦¬ ë¹Œë” |
| **Alembic** | 1.13.1 | ë§ˆì´ê·¸ë ˆì´ì…˜ | SQLAlchemy í†µí•©, ë²„ì „ ê´€ë¦¬ |

### 3.3 Task Queue & Cache
| ê¸°ìˆ  | ë²„ì „ | ìš©ë„ | ì„ íƒ ì´ìœ  |
|------|------|------|-----------|
| **Celery** | 5.3.4 | ë¶„ì‚° ì‘ì—… í | ìŠ¤ì¼€ì¤„ë§, ì¬ì‹œë„, ëª¨ë‹ˆí„°ë§ |
| **Redis** | 5.0.1 | ë©”ì‹œì§€ ë¸Œë¡œì»¤/ìºì‹œ | ë¹ ë¥¸ ì†ë„, Celery í˜¸í™˜ |

### 3.4 Crawling
| ê¸°ìˆ  | ë²„ì „ | ìš©ë„ | ì„ íƒ ì´ìœ  |
|------|------|------|-----------|
| **Playwright** | 1.40.0 | í—¤ë“œë¦¬ìŠ¤ ë¸Œë¼ìš°ì € | JavaScript ë Œë”ë§, ì•ˆì •ì„± |
| **BeautifulSoup4** | 4.12.2 | HTML íŒŒì‹± | ê°„í¸í•œ API, ê°•ë ¥í•œ ì„ íƒì |
| **Requests** | 2.31.0 | HTTP í´ë¼ì´ì–¸íŠ¸ | ì •ì  í˜ì´ì§€ í¬ë¡¤ë§ |
| **lxml** | 4.9.3 | XML/HTML íŒŒì„œ | ë¹ ë¥¸ íŒŒì‹± ì†ë„ |

### 3.5 Monitoring & Logging
| ê¸°ìˆ  | ë²„ì „ | ìš©ë„ | ì„ íƒ ì´ìœ  |
|------|------|------|-----------|
| **Prometheus** | - | ë©”íŠ¸ë¦­ ìˆ˜ì§‘ | ì‹œê³„ì—´ ë°ì´í„°, Grafana í†µí•© |
| **Grafana** | - | ëŒ€ì‹œë³´ë“œ | ì‹œê°í™”, ì•Œë¦¼ |
| **Sentry** | 2.34.1 | ì—ëŸ¬ ì¶”ì  | ì‹¤ì‹œê°„ ì—ëŸ¬ ëª¨ë‹ˆí„°ë§, ì»¨í…ìŠ¤íŠ¸ |

### 3.6 Testing
| ê¸°ìˆ  | ë²„ì „ | ìš©ë„ | ì„ íƒ ì´ìœ  |
|------|------|------|-----------|
| **Pytest** | 7.4.3 | í…ŒìŠ¤íŠ¸ í”„ë ˆì„ì›Œí¬ | ê°•ë ¥í•œ fixture, í”ŒëŸ¬ê·¸ì¸ |
| **Pytest-cov** | 4.1.0 | ì½”ë“œ ì»¤ë²„ë¦¬ì§€ | í…ŒìŠ¤íŠ¸ í’ˆì§ˆ ì¸¡ì • |
| **Pytest-asyncio** | 0.21.1 | ë¹„ë™ê¸° í…ŒìŠ¤íŠ¸ | FastAPI í…ŒìŠ¤íŠ¸ |
| **httpx** | 0.25.2 | HTTP í´ë¼ì´ì–¸íŠ¸ | TestClient ì§€ì› |

### 3.7 Development Tools
| ê¸°ìˆ  | ë²„ì „ | ìš©ë„ | ì„ íƒ ì´ìœ  |
|------|------|------|-----------|
| **Black** | 23.12.1 | ì½”ë“œ í¬ë§·í„° | ì¼ê´€ëœ ìŠ¤íƒ€ì¼ |
| **isort** | 5.13.2 | Import ì •ë ¬ | ê°€ë…ì„± í–¥ìƒ |
| **flake8** | 6.1.0 | ë¦°í„° | ì½”ë“œ í’ˆì§ˆ ê²€ì‚¬ |
| **mypy** | 1.7.1 | íƒ€ì… ì²´ì»¤ | íƒ€ì… ì•ˆì •ì„± |

---

## 4. ë””ë ‰í† ë¦¬ êµ¬ì¡°

### 4.1 ì „ì²´ êµ¬ì¡°

```
College_noti/
â”‚
â”œâ”€â”€ app/                          # ì• í”Œë¦¬ì¼€ì´ì…˜ ë£¨íŠ¸
â”‚   â”‚
â”‚   â”œâ”€â”€ main.py                   # FastAPI ì•± ì§„ì…ì  â­
â”‚   â”œâ”€â”€ api.py                    # API ë¼ìš°í„° ëª¨ìŒ
â”‚   â”œâ”€â”€ config.py                 # ì„¤ì • ê´€ë¦¬ (Pydantic Settings) â­
â”‚   â”œâ”€â”€ models.py                 # SQLAlchemy ëª¨ë¸ â­
â”‚   â”œâ”€â”€ schemas.py                # Pydantic ìŠ¤í‚¤ë§ˆ
â”‚   â”œâ”€â”€ crud.py                   # CRUD ì‘ì—… â­
â”‚   â”œâ”€â”€ database.py               # DB ì—°ê²° ë° ì„¸ì…˜
â”‚   â”œâ”€â”€ tasks.py                  # Celery ì‘ì—… â­
â”‚   â”‚
â”‚   â”œâ”€â”€ college_crawlers.py       # í¬ë¡¤ëŸ¬ êµ¬í˜„ â­
â”‚   â”œâ”€â”€ playwright_crawler.py     # Playwright ë˜í¼
â”‚   â”œâ”€â”€ auto_scheduler.py         # ìë™ ìŠ¤ì¼€ì¤„ëŸ¬
â”‚   â”‚
â”‚   â”œâ”€â”€ circuit_breaker.py        # Circuit Breaker íŒ¨í„´ â­
â”‚   â”œâ”€â”€ rate_limiter.py           # Rate Limiter
â”‚   â”œâ”€â”€ robots_parser.py          # robots.txt íŒŒì„œ
â”‚   â”œâ”€â”€ url_utils.py              # URL ìœ í‹¸ë¦¬í‹°
â”‚   â”‚
â”‚   â”œâ”€â”€ middleware/               # FastAPI ë¯¸ë“¤ì›¨ì–´
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ security.py           # ë³´ì•ˆ ë¯¸ë“¤ì›¨ì–´ â­
â”‚   â”‚   â”œâ”€â”€ rate_limit_middleware.py  # Rate Limiting â­
â”‚   â”‚   â””â”€â”€ metrics_middleware.py     # Metrics ìˆ˜ì§‘
â”‚   â”‚
â”‚   â”œâ”€â”€ metrics.py                # Prometheus ë©”íŠ¸ë¦­
â”‚   â”œâ”€â”€ logging_config.py         # ë¡œê¹… ì„¤ì •
â”‚   â”œâ”€â”€ sentry_config.py          # Sentry ì„¤ì •
â”‚   â”œâ”€â”€ slack_notify.py           # Slack ì•Œë¦¼
â”‚   â”‚
â”‚   â”œâ”€â”€ tests/                    # í…ŒìŠ¤íŠ¸
â”‚   â”‚   â”œâ”€â”€ unit/                 # ë‹¨ìœ„ í…ŒìŠ¤íŠ¸
â”‚   â”‚   â”‚   â””â”€â”€ test_crud.py      # CRUD í…ŒìŠ¤íŠ¸ (21ê°œ)
â”‚   â”‚   â””â”€â”€ integration/          # í†µí•© í…ŒìŠ¤íŠ¸
â”‚   â”‚       â””â”€â”€ test_api_endpoints.py  # API í…ŒìŠ¤íŠ¸ (27ê°œ)
â”‚   â”‚
â”‚   â”œâ”€â”€ .env.example              # í™˜ê²½ ë³€ìˆ˜ í…œí”Œë¦¿ â­
â”‚   â”œâ”€â”€ requirements.txt          # Python ì˜ì¡´ì„±
â”‚   â”œâ”€â”€ Dockerfile                # Docker ì´ë¯¸ì§€
â”‚   â””â”€â”€ worker_app.py             # Celery Worker ì§„ì…ì 
â”‚
â”œâ”€â”€ docker-compose.yml            # Docker Compose ì„¤ì •
â”œâ”€â”€ prometheus.yml                # Prometheus ì„¤ì •
â”œâ”€â”€ grafana/                      # Grafana ì„¤ì •
â”‚   â””â”€â”€ dashboards/
â”‚
â”œâ”€â”€ README.md                     # í”„ë¡œì íŠ¸ README
â””â”€â”€ PROJECT_GUIDELINE.md          # ì´ ë¬¸ì„œ
```

â­ = í•µì‹¬ íŒŒì¼

### 4.2 íŒŒì¼ë³„ ì—­í•  ìš”ì•½

| íŒŒì¼ | ë¼ì¸ ìˆ˜ | ì£¼ìš” ì—­í•  | ì˜ì¡´ì„± |
|------|---------|-----------|--------|
| `main.py` | ~580 | FastAPI ì•± ìƒì„±, ë¯¸ë“¤ì›¨ì–´ ë“±ë¡, ë¼ìš°í„° ë“±ë¡ | api, config, middleware |
| `api.py` | ~620 | API ì—”ë“œí¬ì¸íŠ¸ ì •ì˜ (/jobs, /tasks, /documents) | crud, schemas, models |
| `models.py` | ~100 | SQLAlchemy ORM ëª¨ë¸ (CrawlJob, CrawlTask, CrawlNotice) | database |
| `crud.py` | ~400 | ë°ì´í„°ë² ì´ìŠ¤ CRUD ì‘ì—… | models, database |
| `tasks.py` | ~516 | Celery ë¹„ë™ê¸° ì‘ì—… (í¬ë¡¤ë§, ìŠ¤ì¼€ì¤„ë§) | college_crawlers, crud |
| `college_crawlers.py` | ~432 | ëŒ€í•™ë³„ í¬ë¡¤ëŸ¬ êµ¬í˜„ | playwright, beautifulsoup4 |
| `config.py` | ~358 | ì„¤ì • ê´€ë¦¬ ë° ê²€ì¦ | pydantic-settings |
| `circuit_breaker.py` | ~298 | Circuit Breaker íŒ¨í„´ êµ¬í˜„ | - |

---

## 5. í•µì‹¬ ì»´í¬ë„ŒíŠ¸

### 5.1 API Layer

#### 5.1.1 FastAPI ì• í”Œë¦¬ì¼€ì´ì…˜ (main.py)

**ì—­í• **: ì• í”Œë¦¬ì¼€ì´ì…˜ ì§„ì…ì , ë¯¸ë“¤ì›¨ì–´ ì„¤ì •, ë¼ìš°í„° ë“±ë¡

**ì£¼ìš” ì½”ë“œ êµ¬ì¡°**:
```python
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from fastapi.middleware.trustedhost import TrustedHostMiddleware

# ì•± ìƒì„±
app = FastAPI(
    title="College Notice Crawler API",
    version="1.0.0",
    docs_url="/docs",
    redoc_url="/redoc"
)

# CORS ë¯¸ë“¤ì›¨ì–´
app.add_middleware(
    CORSMiddleware,
    allow_origins=settings.security.allowed_origins.split(","),
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Rate Limiting ë¯¸ë“¤ì›¨ì–´
from middleware.rate_limit_middleware import create_rate_limit_middleware
create_rate_limit_middleware(app)

# Security ë¯¸ë“¤ì›¨ì–´
from middleware.security import SecurityMiddleware
app.add_middleware(SecurityMiddleware)

# ë¼ìš°í„° ë“±ë¡
from api import router
app.include_router(router)
```

**ì£¼ìš” ì—”ë“œí¬ì¸íŠ¸**:
| ì—”ë“œí¬ì¸íŠ¸ | ë©”ì„œë“œ | ì„¤ëª… | ì¸ì¦ í•„ìš” |
|-----------|--------|------|-----------|
| `/health` | GET | í—¬ìŠ¤ ì²´í¬ | âŒ |
| `/jobs` | GET | Job ëª©ë¡ ì¡°íšŒ | âœ… |
| `/jobs` | POST | Job ìƒì„± | âœ… |
| `/jobs/{id}` | GET | Job ì¡°íšŒ | âœ… |
| `/jobs/{id}` | DELETE | Job ì‚­ì œ | âœ… |
| `/jobs/{id}/pause` | POST | Job ì¼ì‹œì •ì§€ | âœ… |
| `/tasks` | GET | Task ëª©ë¡ ì¡°íšŒ | âœ… |
| `/documents` | GET | ê³µì§€ì‚¬í•­ ëª©ë¡ ì¡°íšŒ | âœ… |
| `/documents/search` | GET | ê³µì§€ì‚¬í•­ ê²€ìƒ‰ | âœ… |

#### 5.1.2 API ë¼ìš°í„° (api.py)

**Job ê´€ë¦¬ ì˜ˆì‹œ**:
```python
@router.post("/jobs", response_model=JobResponse)
async def create_job(
    job_data: JobCreate,
    db: Session = Depends(get_db),
    api_key: str = Depends(verify_api_key)
):
    """ìƒˆë¡œìš´ í¬ë¡¤ë§ ì‘ì—… ìƒì„±"""

    # ì¤‘ë³µ ì²´í¬
    existing = crud.get_job_by_name(db, job_data.name)
    if existing:
        raise HTTPException(400, "Job already exists")

    # Job ìƒì„±
    job = crud.create_job(db, job_data.dict())

    # Celery ì‘ì—… ì˜ˆì•½
    if job_data.schedule_type == "cron":
        schedule_cron_job(job.id, job_data.schedule_cron)

    return job
```

**ë¬¸ì„œ ê²€ìƒ‰ ì˜ˆì‹œ**:
```python
@router.get("/documents/search")
async def search_documents(
    q: str,
    category: Optional[str] = None,
    skip: int = 0,
    limit: int = 20,
    db: Session = Depends(get_db),
    api_key: str = Depends(verify_api_key)
):
    """ê³µì§€ì‚¬í•­ ê²€ìƒ‰ (ì œëª©, ë‚´ìš© ì „ë¬¸ ê²€ìƒ‰)"""
    results = crud.search_documents(
        db, q=q, category=category, skip=skip, limit=limit
    )
    return results
```

### 5.2 ë°ì´í„° ëª¨ë¸ (models.py)

#### 5.2.1 CrawlJob (í¬ë¡¤ë§ ì‘ì—…)

```python
class CrawlJob(Base):
    __tablename__ = "crawl_job"

    id = Column(Integer, primary_key=True, index=True)
    name = Column(String(128), nullable=False, unique=True, index=True)
    priority = Column(String(4), nullable=False, index=True)  # P0-P3
    schedule_cron = Column(String(64), nullable=True)
    seed_type = Column(Enum(SeedType), nullable=False)  # URL_LIST, SITEMAP, DOMAIN
    seed_payload = Column(JSON, nullable=False)  # {"urls": [...]}
    render_mode = Column(Enum(RenderMode), nullable=False)  # STATIC, HEADLESS
    rate_limit_per_host = Column(Float, default=1.0)
    max_depth = Column(Integer, default=1)
    robots_policy = Column(Enum(RobotsPolicy), nullable=False)  # OBEY, IGNORE
    status = Column(Enum(JobStatus), default=JobStatus.ACTIVE)

    created_at = Column(DateTime(timezone=True), server_default=func.now())
    updated_at = Column(DateTime(timezone=True), onupdate=func.now())

    # ê´€ê³„
    tasks = relationship("CrawlTask", back_populates="job")
    documents = relationship("CrawlNotice", back_populates="job")
```

**ì£¼ìš” í•„ë“œ ì„¤ëª…**:
- `seed_type`: í¬ë¡¤ë§ ì‹œì‘ì  íƒ€ì…
  - `URL_LIST`: URL ë¦¬ìŠ¤íŠ¸
  - `SITEMAP`: sitemap.xml ê¸°ë°˜
  - `DOMAIN`: ë„ë©”ì¸ ì „ì²´ íƒìƒ‰
- `render_mode`: ë Œë”ë§ ë°©ì‹
  - `STATIC`: HTTP ìš”ì²­ (ë¹ ë¦„)
  - `HEADLESS`: Playwright ë¸Œë¼ìš°ì € (JavaScript ì§€ì›)
- `robots_policy`: robots.txt ì¤€ìˆ˜ ì •ì±…

#### 5.2.2 CrawlTask (ê°œë³„ í¬ë¡¤ë§ íƒœìŠ¤í¬)

```python
class CrawlTask(Base):
    __tablename__ = "crawl_task"

    id = Column(Integer, primary_key=True, index=True)
    job_id = Column(Integer, ForeignKey("crawl_job.id"), nullable=False)
    url = Column(Text, nullable=False, index=True)
    status = Column(Enum(TaskStatus), default=TaskStatus.PENDING)  # PENDING, SUCCESS, FAILED
    retries = Column(Integer, default=0)
    last_error = Column(Text, nullable=True)

    started_at = Column(DateTime(timezone=True), nullable=True)
    finished_at = Column(DateTime(timezone=True), nullable=True)

    http_status = Column(Integer, nullable=True)
    content_hash = Column(String(64), nullable=True, index=True)
    blocked_flag = Column(Boolean, default=False)
    cost_ms_browser = Column(Integer, nullable=True)

    # ê´€ê³„
    job = relationship("CrawlJob", back_populates="tasks")
```

**ìƒíƒœ ì „ì´ ë‹¤ì´ì–´ê·¸ë¨**:
```
PENDING â”€â”€â”€â”€â”€â”€â–º RUNNING â”€â”€â”€â”€â”€â”€â–º SUCCESS
   â”‚               â”‚
   â”‚               â”‚
   â”‚               â–¼
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º FAILED â”€â”€â”€â”€â”€â”€â–º RETRY (max 3íšŒ)
                  â”‚
                  â–¼
              ABANDONED (ì¬ì‹œë„ ì´ˆê³¼)
```

#### 5.2.3 CrawlNotice (ìˆ˜ì§‘ëœ ê³µì§€ì‚¬í•­)

```python
class CrawlNotice(Base):
    __tablename__ = "crawl_notice"

    id = Column(Integer, primary_key=True, index=True)
    job_id = Column(Integer, ForeignKey("crawl_job.id"), nullable=False)

    url = Column(Text, nullable=False, index=True)
    title = Column(Text, nullable=True)
    writer = Column(String(128), nullable=True)
    date = Column(String(32), nullable=True)
    category = Column(String(64), nullable=True, index=True)
    source = Column(String(64), nullable=True)

    extracted = Column(JSON, nullable=True)  # ì¶”ì¶œëœ êµ¬ì¡°í™” ë°ì´í„°
    raw = Column(Text, nullable=True)  # ì›ë³¸ HTML

    fingerprint = Column(String(64), nullable=False, unique=True, index=True)
    snapshot_version = Column(String(32), nullable=True)

    created_at = Column(DateTime(timezone=True), server_default=func.now())

    # ê´€ê³„
    job = relationship("CrawlJob", back_populates="documents")
```

**fingerprint ìƒì„± ë¡œì§**:
```python
import hashlib

def generate_fingerprint(url: str, content: str) -> str:
    """URL + ì»¨í…ì¸  ê¸°ë°˜ ê³ ìœ  ì§€ë¬¸ ìƒì„± (ì¤‘ë³µ ë°©ì§€)"""
    data = f"{url}:{content}".encode('utf-8')
    return hashlib.sha256(data).hexdigest()
```

### 5.3 ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ (crud.py)

#### 5.3.1 Job CRUD

```python
def create_job(db: Session, job_data: Dict[str, Any]) -> CrawlJob:
    """ìƒˆë¡œìš´ í¬ë¡¤ ì¡ ìƒì„±"""
    db_job = CrawlJob(**job_data)
    db.add(db_job)
    db.commit()
    db.refresh(db_job)
    return db_job

def get_job(db: Session, job_id: int) -> Optional[CrawlJob]:
    """IDë¡œ ì¡ ì¡°íšŒ"""
    return db.query(CrawlJob).filter(CrawlJob.id == job_id).first()

def update_job_status(db: Session, job_id: int, status: JobStatus) -> CrawlJob:
    """ì¡ ìƒíƒœ ì—…ë°ì´íŠ¸"""
    job = get_job(db, job_id)
    if not job:
        raise ValueError(f"Job {job_id} not found")

    job.status = status
    job.updated_at = datetime.utcnow()
    db.commit()
    db.refresh(job)
    return job
```

#### 5.3.2 Document CRUD (ë²Œí¬ ìµœì í™”)

```python
def bulk_create_documents(db: Session, docs_data: List[Dict]) -> int:
    """ë¬¸ì„œ ë²Œí¬ ì‚½ì… (ì„±ëŠ¥ ìµœì í™”)"""
    if not docs_data:
        return 0

    # ë²Œí¬ insertë¥¼ ìœ„í•œ ORM ê°ì²´ ë¦¬ìŠ¤íŠ¸
    documents = [CrawlNotice(**doc) for doc in docs_data]

    try:
        db.bulk_save_objects(documents)
        db.commit()
        return len(documents)
    except IntegrityError as e:
        # ì¤‘ë³µ fingerprint ì²˜ë¦¬
        db.rollback()
        logger.warning(f"Duplicate documents found: {e}")
        return 0
```

**ì„±ëŠ¥ ë¹„êµ**:
- ê°œë³„ insert: 100ê°œ ë¬¸ì„œ â†’ ~5ì´ˆ
- ë²Œí¬ insert: 100ê°œ ë¬¸ì„œ â†’ **~0.5ì´ˆ** (10ë°° ë¹ ë¦„)

#### 5.3.3 í†µê³„ ì§‘ê³„

```python
def get_job_statistics(db: Session, job_id: int) -> Dict[str, Any]:
    """ì¡ í†µê³„ ì§‘ê³„"""
    tasks = db.query(CrawlTask).filter(CrawlTask.job_id == job_id).all()

    total = len(tasks)
    completed = sum(1 for t in tasks if t.status == TaskStatus.SUCCESS)
    failed = sum(1 for t in tasks if t.status == TaskStatus.FAILED)

    return {
        "total_tasks": total,
        "completed_tasks": completed,
        "failed_tasks": failed,
        "success_rate": (completed / total * 100) if total > 0 else 0,
        "avg_response_time": sum(t.cost_ms_browser or 0 for t in tasks) / total if total > 0 else 0
    }
```

### 5.4 ë¹„ë™ê¸° ì‘ì—… (tasks.py)

#### 5.4.1 Celery ì„¤ì •

```python
from celery import Celery
from config import get_settings

settings = get_settings()

celery_app = Celery(
    "college_crawler",
    broker=settings.redis.broker_url,
    backend=settings.redis.result_backend
)

celery_app.conf.update(
    task_serializer='json',
    result_serializer='json',
    accept_content=['json'],
    timezone='Asia/Seoul',
    enable_utc=True,

    # ì¬ì‹œë„ ì„¤ì •
    task_acks_late=True,
    task_reject_on_worker_lost=True,

    # Beat ìŠ¤ì¼€ì¤„
    beat_schedule={
        'periodic-crawl': {
            'task': 'tasks.periodic_crawl_all',
            'schedule': crontab(hour='*/6'),  # 6ì‹œê°„ë§ˆë‹¤
        }
    }
)
```

#### 5.4.2 í¬ë¡¤ë§ ì‘ì—…

```python
@celery_app.task(
    bind=True,
    max_retries=3,
    default_retry_delay=60,  # 60ì´ˆ í›„ ì¬ì‹œë„
    autoretry_for=(TemporaryError,)
)
def crawl_task(self, job_id: int, url: str):
    """ê°œë³„ URL í¬ë¡¤ë§ ì‘ì—…"""

    from database import SessionLocal
    from college_crawlers import get_college_crawler

    db = SessionLocal()

    try:
        # Job ì¡°íšŒ
        job = crud.get_job(db, job_id)
        if not job:
            raise ValueError(f"Job {job_id} not found")

        # í¬ë¡¤ëŸ¬ ì„ íƒ
        crawler = get_college_crawler(job.name)

        # Circuit Breaker ì ìš©
        with CircuitBreaker(name=f"crawler_{job.name}"):
            # í¬ë¡¤ë§ ì‹¤í–‰
            results = crawler.crawl(url)

        # ê²°ê³¼ ì €ì¥
        saved = crud.bulk_create_documents(db, results)

        logger.info(f"Crawled {url}: {saved} documents saved")

        return {"status": "success", "documents": saved}

    except CircuitBreakerOpenError:
        # Circuit Breaker ì—´ë¦¼ - ì¼ì‹œì  ì‹¤íŒ¨
        logger.warning(f"Circuit breaker open for {job.name}")
        raise self.retry(countdown=300)  # 5ë¶„ í›„ ì¬ì‹œë„

    except Exception as e:
        logger.error(f"Crawl failed for {url}: {e}")
        raise

    finally:
        db.close()
```

**ì¬ì‹œë„ ì „ëµ (Exponential Backoff + Jitter)**:
```python
def exponential_backoff_with_jitter(retry_count: int, base_delay: int = 60) -> int:
    """
    ì§€ìˆ˜ ë°±ì˜¤í”„ + Jitter

    retry_count=0: 60s
    retry_count=1: 120s + random(0-30s)
    retry_count=2: 240s + random(0-60s)
    """
    import random

    delay = base_delay * (2 ** retry_count)
    jitter = random.randint(0, delay // 2)
    return delay + jitter
```

### 5.5 í¬ë¡¤ëŸ¬ ì—”ì§„ (college_crawlers.py)

#### 5.5.1 BaseCrawler (ì¶”ìƒ í´ë˜ìŠ¤)

```python
from abc import ABC, abstractmethod
from typing import List, Dict

class BaseCrawler(ABC):
    """ëª¨ë“  í¬ë¡¤ëŸ¬ì˜ ê¸°ë³¸ í´ë˜ìŠ¤"""

    def __init__(self, rate_limit: float = 1.0):
        self.rate_limit = rate_limit  # ìš”ì²­ ê°„ ìµœì†Œ ê°„ê²© (ì´ˆ)
        self.last_request_time = 0

    @abstractmethod
    async def crawl(self, url: str) -> List[Dict]:
        """
        í¬ë¡¤ë§ ì‹¤í–‰ (ë°˜ë“œì‹œ êµ¬í˜„ í•„ìš”)

        Returns:
            List[Dict]: ê³µì§€ì‚¬í•­ ë¦¬ìŠ¤íŠ¸
                [
                    {
                        "title": "ê³µì§€ì‚¬í•­ ì œëª©",
                        "url": "https://...",
                        "date": "2025-11-03",
                        "category": "scholarship",
                        ...
                    }
                ]
        """
        pass

    def respect_rate_limit(self):
        """Rate Limit ì¤€ìˆ˜"""
        import time

        elapsed = time.time() - self.last_request_time
        if elapsed < self.rate_limit:
            time.sleep(self.rate_limit - elapsed)

        self.last_request_time = time.time()

    async def fetch_html(self, url: str, use_browser: bool = False) -> str:
        """HTML ê°€ì ¸ì˜¤ê¸°"""
        self.respect_rate_limit()

        if use_browser:
            # Playwright ì‚¬ìš© (JavaScript ë Œë”ë§)
            return await self._fetch_with_playwright(url)
        else:
            # Requests ì‚¬ìš© (ì •ì  í˜ì´ì§€)
            return self._fetch_with_requests(url)
```

#### 5.5.2 ëŒ€í•™ë³„ í¬ë¡¤ëŸ¬ êµ¬í˜„ ì˜ˆì‹œ

```python
class KonkukCrawler(BaseCrawler):
    """ê±´êµ­ëŒ€í•™êµ í¬ë¡¤ëŸ¬"""

    BASE_URL = "https://www.konkuk.ac.kr"
    NOTICE_URL = f"{BASE_URL}/ko/board/notice"

    async def crawl(self, url: str) -> List[Dict]:
        """ê±´êµ­ëŒ€ ê³µì§€ì‚¬í•­ í¬ë¡¤ë§"""

        # HTML ê°€ì ¸ì˜¤ê¸°
        html = await self.fetch_html(url, use_browser=False)

        # BeautifulSoupìœ¼ë¡œ íŒŒì‹±
        soup = BeautifulSoup(html, 'lxml')

        # ê³µì§€ì‚¬í•­ ë¦¬ìŠ¤íŠ¸ ì¶”ì¶œ
        notices = []
        for item in soup.select('.board-list tr'):
            try:
                title_elem = item.select_one('.title')
                date_elem = item.select_one('.date')

                if not title_elem:
                    continue

                notice = {
                    "title": title_elem.text.strip(),
                    "url": urljoin(self.BASE_URL, title_elem['href']),
                    "date": date_elem.text.strip() if date_elem else None,
                    "category": self._extract_category(item),
                    "source": "konkuk",
                    "fingerprint": self._generate_fingerprint(title_elem['href'])
                }

                notices.append(notice)

            except Exception as e:
                logger.warning(f"Failed to parse item: {e}")
                continue

        return notices

    def _extract_category(self, item) -> str:
        """ì¹´í…Œê³ ë¦¬ ì¶”ì¶œ ë¡œì§"""
        category_elem = item.select_one('.category')
        if category_elem:
            text = category_elem.text.strip()
            if 'ì¥í•™' in text:
                return 'scholarship'
            elif 'í•™ì‚¬' in text:
                return 'academic'
        return 'general'
```

**í¬ë¡¤ëŸ¬ ë“±ë¡**:
```python
# college_crawlers.py í•˜ë‹¨
COLLEGE_CRAWLERS = {
    "konkuk": KonkukCrawler,
    "seoultech": SeoulTechCrawler,
    "korea": KoreaUnivCrawler,
    # ... í™•ì¥ ê°€ëŠ¥
}

def get_college_crawler(name: str) -> BaseCrawler:
    """í¬ë¡¤ëŸ¬ íŒ©í† ë¦¬ í•¨ìˆ˜"""
    crawler_class = COLLEGE_CRAWLERS.get(name)
    if not crawler_class:
        raise ValueError(f"Unknown crawler: {name}")

    return crawler_class()
```

### 5.6 ì•ˆì •ì„± íŒ¨í„´

#### 5.6.1 Circuit Breaker

```python
class CircuitBreaker:
    """
    Circuit Breaker íŒ¨í„´ êµ¬í˜„

    ìƒíƒœ:
    - CLOSED: ì •ìƒ ë™ì‘
    - OPEN: ì°¨ë‹¨ (ì—°ì† ì‹¤íŒ¨ ì„ê³„ê°’ ì´ˆê³¼)
    - HALF_OPEN: ë³µêµ¬ ì‹œë„
    """

    def __init__(self,
                 name: str,
                 failure_threshold: int = 5,
                 timeout: int = 60,
                 success_threshold: int = 2):
        self.name = name
        self.failure_threshold = failure_threshold
        self.timeout = timeout
        self.success_threshold = success_threshold

        self.state = "CLOSED"
        self.failure_count = 0
        self.success_count = 0
        self.last_failure_time = None

    def call(self, func, *args, **kwargs):
        """í•¨ìˆ˜ ì‹¤í–‰ with Circuit Breaker"""

        if self.state == "OPEN":
            # Circuitì´ ì—´ë¦° ìƒíƒœ
            if time.time() - self.last_failure_time >= self.timeout:
                # íƒ€ì„ì•„ì›ƒ ê²½ê³¼ â†’ HALF_OPENìœ¼ë¡œ ì „í™˜
                self.state = "HALF_OPEN"
                logger.info(f"Circuit {self.name}: OPEN â†’ HALF_OPEN")
            else:
                # ì•„ì§ ì°¨ë‹¨ ìƒíƒœ
                raise CircuitBreakerOpenError(f"Circuit {self.name} is OPEN")

        try:
            # í•¨ìˆ˜ ì‹¤í–‰
            result = func(*args, **kwargs)

            # ì„±ê³µ ì²˜ë¦¬
            self.on_success()

            return result

        except Exception as e:
            # ì‹¤íŒ¨ ì²˜ë¦¬
            self.on_failure()
            raise

    def on_success(self):
        """ì„±ê³µ ì‹œ ì²˜ë¦¬"""
        self.failure_count = 0

        if self.state == "HALF_OPEN":
            self.success_count += 1

            if self.success_count >= self.success_threshold:
                # ë³µêµ¬ ì„±ê³µ â†’ CLOSED
                self.state = "CLOSED"
                self.success_count = 0
                logger.info(f"Circuit {self.name}: HALF_OPEN â†’ CLOSED")

    def on_failure(self):
        """ì‹¤íŒ¨ ì‹œ ì²˜ë¦¬"""
        self.failure_count += 1
        self.last_failure_time = time.time()

        if self.failure_count >= self.failure_threshold:
            # ì„ê³„ê°’ ì´ˆê³¼ â†’ OPEN
            self.state = "OPEN"
            logger.warning(f"Circuit {self.name}: CLOSED â†’ OPEN")
```

**ì‚¬ìš© ì˜ˆì‹œ**:
```python
breaker = CircuitBreaker(name="external_api", failure_threshold=3, timeout=60)

try:
    result = breaker.call(requests.get, "https://api.example.com/data")
except CircuitBreakerOpenError:
    # Circuitì´ ì—´ë¦¼ â†’ ëŒ€ì²´ ë¡œì§
    result = get_cached_data()
```

#### 5.6.2 Rate Limiter (Token Bucket)

```python
class TokenBucketRateLimiter:
    """
    Token Bucket ì•Œê³ ë¦¬ì¦˜ ê¸°ë°˜ Rate Limiter

    - ì´ˆë‹¹ Nê°œì˜ í† í° ìƒì„±
    - ìµœëŒ€ Mê°œì˜ í† í° ë³´ìœ  ê°€ëŠ¥
    - ìš”ì²­ ì‹œ í† í° 1ê°œ ì†Œë¹„
    """

    def __init__(self, rate: float, capacity: int):
        """
        Args:
            rate: ì´ˆë‹¹ í† í° ìƒì„± ì†ë„
            capacity: ìµœëŒ€ í† í° ìš©ëŸ‰
        """
        self.rate = rate
        self.capacity = capacity
        self.tokens = capacity
        self.last_update = time.time()
        self.lock = threading.Lock()

    def acquire(self, tokens: int = 1) -> bool:
        """í† í° íšë“ ì‹œë„"""
        with self.lock:
            self._refill()

            if self.tokens >= tokens:
                self.tokens -= tokens
                return True
            else:
                return False

    def _refill(self):
        """í† í° ì¬ì¶©ì „"""
        now = time.time()
        elapsed = now - self.last_update

        # ê²½ê³¼ ì‹œê°„ì— ë¹„ë¡€í•˜ì—¬ í† í° ì¶”ê°€
        new_tokens = elapsed * self.rate
        self.tokens = min(self.capacity, self.tokens + new_tokens)

        self.last_update = now
```

**ì‚¬ìš© ì˜ˆì‹œ**:
```python
# ì´ˆë‹¹ 2ê°œ ìš”ì²­ í—ˆìš©
limiter = TokenBucketRateLimiter(rate=2.0, capacity=10)

if limiter.acquire():
    # ìš”ì²­ ì‹¤í–‰
    make_request()
else:
    # Rate limit ì´ˆê³¼
    raise RateLimitExceeded("Too many requests")
```

### 5.7 ë³´ì•ˆ (middleware/security.py)

#### 5.7.1 API Key ì¸ì¦

```python
from fastapi import Security, HTTPException
from fastapi.security import APIKeyHeader

api_key_header = APIKeyHeader(name="X-API-Key", auto_error=False)

async def verify_api_key(api_key: str = Security(api_key_header)):
    """API í‚¤ ê²€ì¦"""

    settings = get_settings()

    if not api_key:
        raise HTTPException(
            status_code=401,
            detail="API Key is required",
            headers={"WWW-Authenticate": "ApiKey"}
        )

    if api_key != settings.security.api_key:
        raise HTTPException(
            status_code=403,
            detail="Invalid API Key"
        )

    return api_key
```

**ì‚¬ìš© ì˜ˆì‹œ**:
```python
@router.get("/protected")
async def protected_route(api_key: str = Depends(verify_api_key)):
    """API í‚¤ê°€ í•„ìš”í•œ ì—”ë“œí¬ì¸íŠ¸"""
    return {"message": "Access granted"}
```

#### 5.7.2 Rate Limiting Middleware

```python
class RateLimitMiddleware(BaseHTTPMiddleware):
    """Redis ê¸°ë°˜ ë¶„ì‚° Rate Limiting"""

    async def dispatch(self, request: Request, call_next):
        # API í‚¤ ë˜ëŠ” IP ì£¼ì†Œë¡œ í´ë¼ì´ì–¸íŠ¸ ì‹ë³„
        client_id = self._get_client_id(request)

        # Rate limit í™•ì¸
        allowed, remaining, reset_time = await self._check_rate_limit(client_id)

        if not allowed:
            return JSONResponse(
                status_code=429,
                content={"error": "Rate limit exceeded"},
                headers={
                    "X-RateLimit-Limit": str(self.max_requests),
                    "X-RateLimit-Remaining": "0",
                    "X-RateLimit-Reset": str(reset_time),
                    "Retry-After": str(reset_time)
                }
            )

        # ìš”ì²­ ì²˜ë¦¬
        response = await call_next(request)

        # Rate limit í—¤ë” ì¶”ê°€
        response.headers["X-RateLimit-Limit"] = str(self.max_requests)
        response.headers["X-RateLimit-Remaining"] = str(remaining)

        return response
```

### 5.8 ëª¨ë‹ˆí„°ë§ (metrics.py, sentry_config.py)

#### 5.8.1 Prometheus ë©”íŠ¸ë¦­

```python
from prometheus_client import Counter, Histogram, Gauge

# HTTP ìš”ì²­ ì¹´ìš´í„°
http_requests_total = Counter(
    'http_requests_total',
    'Total HTTP requests',
    ['method', 'endpoint', 'status']
)

# ì‘ë‹µ ì‹œê°„ íˆìŠ¤í† ê·¸ë¨
http_request_duration_seconds = Histogram(
    'http_request_duration_seconds',
    'HTTP request duration',
    ['method', 'endpoint']
)

# í™œì„± ì‘ì—… ê²Œì´ì§€
active_tasks = Gauge(
    'active_crawl_tasks',
    'Number of active crawl tasks'
)

# í¬ë¡¤ë§ ì„±ê³µ/ì‹¤íŒ¨ ì¹´ìš´í„°
crawl_results_total = Counter(
    'crawl_results_total',
    'Total crawl results',
    ['status', 'college']
)
```

**ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ì˜ˆì‹œ**:
```python
@router.get("/api/data")
async def get_data():
    # ìš”ì²­ ì¹´ìš´íŠ¸
    http_requests_total.labels(method="GET", endpoint="/api/data", status="200").inc()

    # ì‘ë‹µ ì‹œê°„ ì¸¡ì •
    with http_request_duration_seconds.labels(method="GET", endpoint="/api/data").time():
        data = fetch_data()

    return data
```

#### 5.8.2 Sentry ì—ëŸ¬ ì¶”ì 

```python
import sentry_sdk
from sentry_sdk.integrations.fastapi import FastApiIntegration
from sentry_sdk.integrations.celery import CeleryIntegration

def init_sentry():
    """Sentry ì´ˆê¸°í™”"""
    settings = get_settings()

    if not settings.monitoring.sentry_dsn:
        return

    sentry_sdk.init(
        dsn=settings.monitoring.sentry_dsn,
        environment=settings.monitoring.environment,
        integrations=[
            FastApiIntegration(),
            CeleryIntegration()
        ],
        traces_sample_rate=0.1,  # 10% íŠ¸ëœì­ì…˜ ìƒ˜í”Œë§
        profiles_sample_rate=0.1
    )
```

**ì—ëŸ¬ ì»¨í…ìŠ¤íŠ¸ ì¶”ê°€**:
```python
from sentry_sdk import capture_exception, set_context

try:
    result = crawl_website(url)
except Exception as e:
    # ì»¨í…ìŠ¤íŠ¸ ì¶”ê°€
    set_context("crawl_job", {
        "job_id": job_id,
        "url": url,
        "crawler": crawler_name
    })

    # Sentryë¡œ ì „ì†¡
    capture_exception(e)

    raise
```

---

## 6. ë°ì´í„° íë¦„

### 6.1 í¬ë¡¤ë§ ì‘ì—… ìƒì„± íë¦„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Client  â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
     â”‚ POST /jobs
     â”‚ {
     â”‚   "name": "konkuk_scholarship",
     â”‚   "seed_type": "URL_LIST",
     â”‚   "seed_payload": {"urls": [...]},
     â”‚   "schedule_cron": "0 */6 * * *"
     â”‚ }
     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  FastAPI (main.py)                  â”‚
â”‚  1. API Key ê²€ì¦                    â”‚
â”‚  2. Rate Limit í™•ì¸                 â”‚
â”‚  3. Pydantic ìŠ¤í‚¤ë§ˆ ê²€ì¦            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  API Router (api.py)                â”‚
â”‚  1. ì¤‘ë³µ Job í™•ì¸                   â”‚
â”‚  2. crud.create_job() í˜¸ì¶œ          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  CRUD (crud.py)                     â”‚
â”‚  1. ORM ê°ì²´ ìƒì„±                   â”‚
â”‚  2. DBì— INSERT                     â”‚
â”‚  3. commit & refresh                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PostgreSQL                         â”‚
â”‚  INSERT INTO crawl_job VALUES(...)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â”‚ (Job ID ë°˜í™˜)
             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Celery Beat Scheduler              â”‚
â”‚  1. cron í‘œí˜„ì‹ íŒŒì‹±                â”‚
â”‚  2. ìŠ¤ì¼€ì¤„ ë“±ë¡                     â”‚
â”‚  3. ì£¼ê¸°ì  ì‘ì—… íŠ¸ë¦¬ê±°              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 6.2 í¬ë¡¤ë§ ì‹¤í–‰ íë¦„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Celery Beat        â”‚
â”‚  (ë§¤ 6ì‹œê°„ë§ˆë‹¤)     â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚ tasks.periodic_crawl_all.delay()
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Redis (Message Queue)              â”‚
â”‚  Task Queueì— ë©”ì‹œì§€ ì¶”ê°€           â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â”‚ Workerê°€ ë©”ì‹œì§€ íšë“
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Celery Worker                      â”‚
â”‚  tasks.crawl_task(job_id, url)      â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  1. Job ì¡°íšŒ (DB)                   â”‚
â”‚  job = crud.get_job(db, job_id)     â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  2. í¬ë¡¤ëŸ¬ ì„ íƒ                     â”‚
â”‚  crawler = get_college_crawler(     â”‚
â”‚      job.name                        â”‚
â”‚  )                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  3. Circuit Breaker í™•ì¸            â”‚
â”‚  with CircuitBreaker(...):          â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  4. ì›¹ì‚¬ì´íŠ¸ ì ‘ì†                   â”‚
â”‚  - robots.txt í™•ì¸                  â”‚
â”‚  - Rate Limiting ì ìš©               â”‚
â”‚  - HTML ë‹¤ìš´ë¡œë“œ                    â”‚
â”‚    (Playwright or Requests)         â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  5. HTML íŒŒì‹±                       â”‚
â”‚  soup = BeautifulSoup(html)         â”‚
â”‚  notices = []                        â”‚
â”‚  for item in soup.select(...):      â”‚
â”‚      notices.append({...})           â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  6. ì¤‘ë³µ ì œê±°                       â”‚
â”‚  for notice in notices:              â”‚
â”‚      fingerprint = generate(...)     â”‚
â”‚      if exists(fingerprint):         â”‚
â”‚          skip                        â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  7. ë°ì´í„° ì €ì¥ (ë²Œí¬)              â”‚
â”‚  saved = crud.bulk_create_documents( â”‚
â”‚      db, notices                     â”‚
â”‚  )                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PostgreSQL                         â”‚
â”‚  BULK INSERT INTO crawl_notice      â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  8. Task ìƒíƒœ ì—…ë°ì´íŠ¸              â”‚
â”‚  crud.update_task_status(           â”‚
â”‚      task_id, SUCCESS               â”‚
â”‚  )                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  9. ì›¹í›… ì•Œë¦¼ (ì„ íƒ)                â”‚
â”‚  send_webhook(job_id, result)       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 6.3 ê³µì§€ì‚¬í•­ ì¡°íšŒ íë¦„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Client  â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
     â”‚ GET /documents?category=scholarship&limit=20
     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  FastAPI Middleware                 â”‚
â”‚  1. Rate Limit í™•ì¸                 â”‚
â”‚  2. API Key ê²€ì¦                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  API Router                         â”‚
â”‚  crud.get_documents(                â”‚
â”‚      category="scholarship",         â”‚
â”‚      limit=20                        â”‚
â”‚  )                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  CRUD                               â”‚
â”‚  query = db.query(CrawlNotice)      â”‚
â”‚  if category:                        â”‚
â”‚      query.filter(category=category)â”‚
â”‚  query.limit(20)                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PostgreSQL                         â”‚
â”‚  SELECT * FROM crawl_notice         â”‚
â”‚  WHERE category = 'scholarship'     â”‚
â”‚  LIMIT 20                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â”‚ (ê²°ê³¼ ë°˜í™˜)
             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Pydantic Schema ë³€í™˜               â”‚
â”‚  [DocumentResponse(...) for ...]    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â”‚ JSON ì‘ë‹µ
             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Client  â”‚
â”‚  {        â”‚
â”‚    "data": [                         â”‚
â”‚      {                                â”‚
â”‚        "id": 1,                       â”‚
â”‚        "title": "...",                â”‚
â”‚        "url": "...",                  â”‚
â”‚        ...                            â”‚
â”‚      }                                â”‚
â”‚    ]                                  â”‚
â”‚  }        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 7. ì„¤ì • ë° í™˜ê²½

### 7.1 Configuration êµ¬ì¡° (config.py)

**ê³„ì¸µ êµ¬ì¡°**:
```
Settings (ìµœìƒìœ„)
â”œâ”€â”€ database: DatabaseSettings
â”‚   â”œâ”€â”€ url
â”‚   â”œâ”€â”€ pool_size
â”‚   â””â”€â”€ max_overflow
â”œâ”€â”€ redis: RedisSettings
â”‚   â”œâ”€â”€ host
â”‚   â”œâ”€â”€ port
â”‚   â””â”€â”€ password
â”œâ”€â”€ crawler: CrawlerSettings
â”‚   â”œâ”€â”€ default_rate_limit_per_host
â”‚   â””â”€â”€ max_concurrent_requests_per_host
â”œâ”€â”€ playwright: PlaywrightSettings
â”‚   â”œâ”€â”€ browser_type
â”‚   â””â”€â”€ headless
â”œâ”€â”€ monitoring: MonitoringSettings
â”‚   â”œâ”€â”€ sentry_dsn
â”‚   â””â”€â”€ log_level
â”œâ”€â”€ notification: NotificationSettings
â”‚   â””â”€â”€ slack_webhook_url
â””â”€â”€ security: SecuritySettings
    â”œâ”€â”€ api_key
    â”œâ”€â”€ allowed_origins
    â””â”€â”€ enable_rate_limiting
```

**ì„¤ì • ë¡œë”© ìˆœì„œ**:
```
1. ê¸°ë³¸ê°’ (config.pyì˜ Field default)
   â†“
2. .env íŒŒì¼
   â†“
3. í™˜ê²½ ë³€ìˆ˜ (ìš°ì„ ìˆœìœ„ ë†’ìŒ)
   â†“
4. validate_settings() ê²€ì¦
   â†“
5. ì• í”Œë¦¬ì¼€ì´ì…˜ì—ì„œ ì‚¬ìš©
```

### 7.2 í™˜ê²½ ë³€ìˆ˜ ìš°ì„ ìˆœìœ„

**í”„ë¡œë•ì…˜ vs ê°œë°œ í™˜ê²½**:

| í•­ëª© | ê°œë°œ í™˜ê²½ | í”„ë¡œë•ì…˜ í™˜ê²½ |
|------|-----------|---------------|
| `ENV` | development | production |
| `DEBUG` | true | false |
| `SECRET_KEY` | 32ì+ | **64ì+ í•„ìˆ˜** |
| `API_KEY` | 16ì+ | **32ì+ í•„ìˆ˜** |
| `DATABASE_URL` | crawler123 í—ˆìš© | **ê°•ë ¥í•œ ë¹„ë°€ë²ˆí˜¸ í•„ìˆ˜** |
| `LOG_LEVEL` | DEBUG | WARNING/ERROR |
| `SENTRY_DSN` | ì„ íƒ | **í•„ìˆ˜** |
| `CORS` | * í—ˆìš© | íŠ¹ì • ë„ë©”ì¸ë§Œ |

**ê²€ì¦ ë¡œì§** (config.py:250-351):
```python
def validate_settings():
    """ì„¤ì • ìœ íš¨ì„± ê²€ì‚¬"""
    settings = get_settings()

    # SECRET_KEY ê²€ì¦
    if settings.monitoring.environment == "production":
        if len(settings.secret_key) < 64:
            raise ValueError("âŒ SECRET_KEY must be at least 64 characters in production")

        # ì·¨ì•½í•œ í‚¤ ê°ì§€
        insecure_keys = ["your-secret-key-here", "dev-secret-key", "test", "password"]
        if any(weak in settings.secret_key.lower() for weak in insecure_keys):
            raise ValueError("âŒ Cannot use weak SECRET_KEY in production")

        # ë°ì´í„°ë² ì´ìŠ¤ ë¹„ë°€ë²ˆí˜¸ ê²€ì¦
        weak_passwords = ["crawler123", "password", "admin", "12345"]
        db_url_lower = settings.database.url.lower()
        if any(weak in db_url_lower for weak in weak_passwords):
            raise ValueError("âŒ Weak database password detected in production")
```

### 7.3 ì£¼ìš” í™˜ê²½ ë³€ìˆ˜

| í™˜ê²½ ë³€ìˆ˜ | ì„¤ëª… | ê¸°ë³¸ê°’ | í”„ë¡œë•ì…˜ í•„ìˆ˜ |
|-----------|------|--------|---------------|
| `DATABASE_URL` | PostgreSQL ì—°ê²° ë¬¸ìì—´ | `postgresql://crawler:crawler123@postgres:5432/school_notices` | âœ… |
| `CELERY_BROKER_URL` | Redis ë¸Œë¡œì»¤ URL | `redis://redis:6379/0` | âœ… |
| `SECRET_KEY` | ì• í”Œë¦¬ì¼€ì´ì…˜ ì•”í˜¸í™” í‚¤ | - | âœ… |
| `API_KEY` | API ì¸ì¦ í‚¤ | - | âœ… |
| `SENTRY_DSN` | Sentry DSN | - | âœ… |
| `LOG_LEVEL` | ë¡œê·¸ ë ˆë²¨ | `INFO` | âŒ |
| `ENABLE_RATE_LIMITING` | Rate Limiting í™œì„±í™” | `true` | âœ… |
| `MAX_REQUESTS_PER_MINUTE` | ë¶„ë‹¹ ìµœëŒ€ ìš”ì²­ ìˆ˜ | `60` | âŒ |

---

## 8. í…ŒìŠ¤íŠ¸ ì „ëµ

### 8.1 í…ŒìŠ¤íŠ¸ êµ¬ì¡°

```
tests/
â”œâ”€â”€ unit/                    # ë‹¨ìœ„ í…ŒìŠ¤íŠ¸
â”‚   â””â”€â”€ test_crud.py         # CRUD í•¨ìˆ˜ í…ŒìŠ¤íŠ¸ (21ê°œ)
â”‚       â”œâ”€â”€ TestJobCRUD      # Job CRUD (7ê°œ)
â”‚       â”œâ”€â”€ TestTaskCRUD     # Task CRUD (5ê°œ)
â”‚       â”œâ”€â”€ TestDocumentCRUD # Document CRUD (6ê°œ)
â”‚       â”œâ”€â”€ TestStatistics   # í†µê³„ (1ê°œ)
â”‚       â””â”€â”€ TestBulkOperations (2ê°œ)
â”‚
â””â”€â”€ integration/             # í†µí•© í…ŒìŠ¤íŠ¸
    â””â”€â”€ test_api_endpoints.py  # API ì—”ë“œí¬ì¸íŠ¸ (27ê°œ)
        â”œâ”€â”€ TestHealthCheck  # í—¬ìŠ¤ ì²´í¬ (1ê°œ)
        â”œâ”€â”€ TestJobEndpoints # Job API (7ê°œ)
        â”œâ”€â”€ TestTaskEndpoints # Task API (2ê°œ)
        â”œâ”€â”€ TestDocumentEndpoints # Document API (4ê°œ)
        â”œâ”€â”€ TestStatisticsEndpoints (1ê°œ)
        â”œâ”€â”€ TestRateLimiting # Rate Limiting (1ê°œ)
        â””â”€â”€ TestErrorHandling # ì—ëŸ¬ ì²˜ë¦¬ (3ê°œ)
```

### 8.2 í…ŒìŠ¤íŠ¸ ì»¤ë²„ë¦¬ì§€

**í˜„ì¬ ì»¤ë²„ë¦¬ì§€**: 19%

| ëª¨ë“ˆ | ë¼ì¸ ìˆ˜ | ì»¤ë²„ | ë¹„ìœ¨ |
|------|---------|------|------|
| config.py | 145 | 93 | 64% |
| crud.py | 164 | 112 | 68% |
| models.py | 101 | 101 | **100%** |
| database.py | 38 | 17 | 45% |
| tasks.py | 246 | 0 | 0% |
| college_crawlers.py | 242 | 0 | 0% |

**ëª©í‘œ ì»¤ë²„ë¦¬ì§€**: 80%

**ìš°ì„ ìˆœìœ„**:
1. âœ… models.py (100%) - ì™„ë£Œ
2. âœ… crud.py (68%) - ì™„ë£Œ
3. âœ… config.py (64%) - ì™„ë£Œ
4. â³ tasks.py (0% â†’ 70%) - ë‹¤ìŒ ëª©í‘œ
5. â³ college_crawlers.py (0% â†’ 60%) - ë‹¤ìŒ ëª©í‘œ

### 8.3 í…ŒìŠ¤íŠ¸ ì‹¤í–‰

```bash
# ì „ì²´ í…ŒìŠ¤íŠ¸
PYTHONPATH=. pytest

# ì»¤ë²„ë¦¬ì§€ í¬í•¨
PYTHONPATH=. pytest --cov=. --cov-report=html

# íŠ¹ì • í…ŒìŠ¤íŠ¸ íŒŒì¼
PYTHONPATH=. pytest tests/unit/test_crud.py -v

# íŠ¹ì • í…ŒìŠ¤íŠ¸ í´ë˜ìŠ¤
PYTHONPATH=. pytest tests/unit/test_crud.py::TestJobCRUD -v

# ë§ˆì»¤ ê¸°ë°˜
PYTHONPATH=. pytest -m "unit" -v
```

### 8.4 í…ŒìŠ¤íŠ¸ ì˜ˆì‹œ

#### ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ (test_crud.py)
```python
def test_create_job(db_session):
    """ì¡ ìƒì„± í…ŒìŠ¤íŠ¸"""
    job_data = {
        "name": "test-job",
        "priority": "P1",
        "seed_type": "URL_LIST",
        "seed_payload": {"urls": ["https://example.com"]},
        "render_mode": "STATIC",
        "robots_policy": "OBEY",
    }

    job = crud.create_job(db_session, job_data)

    assert job.id is not None
    assert job.name == job_data["name"]
    assert job.status == JobStatus.ACTIVE
```

#### í†µí•© í…ŒìŠ¤íŠ¸ (test_api_endpoints.py)
```python
def test_create_job_api(client, api_key):
    """Job ìƒì„± API í…ŒìŠ¤íŠ¸"""
    response = client.post(
        "/jobs",
        json={
            "name": "api-test-job",
            "priority": "P1",
            "seed_type": "URL_LIST",
            "seed_payload": {"urls": ["https://example.com"]}
        },
        headers={"X-API-Key": api_key}
    )

    assert response.status_code == 200
    data = response.json()
    assert data["name"] == "api-test-job"
    assert "id" in data
```

---

## 9. ë°°í¬ ë° ìš´ì˜

### 9.1 Docker Compose êµ¬ì„±

```yaml
version: '3.8'

services:
  # FastAPI ì„œë²„
  app:
    build: ./app
    ports:
      - "8000:8000"
    environment:
      - DATABASE_URL=${DATABASE_URL}
      - CELERY_BROKER_URL=${CELERY_BROKER_URL}
      - API_KEY=${API_KEY}
    depends_on:
      - postgres
      - redis
    volumes:
      - ./app:/app

  # Celery Worker
  celery_worker:
    build: ./app
    command: celery -A worker_app worker --loglevel=info
    environment:
      - CELERY_BROKER_URL=${CELERY_BROKER_URL}
    depends_on:
      - redis
      - postgres

  # Celery Beat (ìŠ¤ì¼€ì¤„ëŸ¬)
  celery_beat:
    build: ./app
    command: celery -A worker_app beat --loglevel=info
    environment:
      - CELERY_BROKER_URL=${CELERY_BROKER_URL}
    depends_on:
      - redis

  # PostgreSQL
  postgres:
    image: postgres:15
    environment:
      - POSTGRES_USER=crawler
      - POSTGRES_PASSWORD=${DB_PASSWORD}
      - POSTGRES_DB=school_notices
    volumes:
      - postgres_data:/var/lib/postgresql/data
    ports:
      - "5432:5432"

  # Redis
  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"

  # Prometheus
  prometheus:
    image: prom/prometheus
    volumes:
      - ./prometheus.yml:/etc/prometheus/prometheus.yml
    ports:
      - "9090:9090"

  # Grafana
  grafana:
    image: grafana/grafana
    ports:
      - "3000:3000"
    volumes:
      - grafana_data:/var/lib/grafana

volumes:
  postgres_data:
  grafana_data:
```

### 9.2 ë°°í¬ ì²´í¬ë¦¬ìŠ¤íŠ¸

#### ì‚¬ì „ ì¤€ë¹„
- [ ] `.env` íŒŒì¼ ìƒì„± ë° ëª¨ë“  í•„ìˆ˜ í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
- [ ] `SECRET_KEY` ìƒì„± (64ì ì´ìƒ)
- [ ] `API_KEY` ìƒì„± (32ì ì´ìƒ)
- [ ] ë°ì´í„°ë² ì´ìŠ¤ ë¹„ë°€ë²ˆí˜¸ ë³€ê²½
- [ ] `ENV=production` ì„¤ì •
- [ ] `DEBUG=false` í™•ì¸

#### ë³´ì•ˆ
- [ ] HTTPS ì„¤ì • (Let's Encrypt)
- [ ] `ALLOWED_ORIGINS` ì‹¤ì œ ë„ë©”ì¸ìœ¼ë¡œ ì œí•œ
- [ ] `TRUSTED_HOSTS` ì„¤ì •
- [ ] Rate Limiting í™œì„±í™”
- [ ] Firewall ì„¤ì • (í¬íŠ¸ ì œí•œ)

#### ëª¨ë‹ˆí„°ë§
- [ ] Sentry DSN ì„¤ì •
- [ ] Prometheus ë©”íŠ¸ë¦­ í™•ì¸
- [ ] Grafana ëŒ€ì‹œë³´ë“œ êµ¬ì„±
- [ ] ë¡œê·¸ ìˆ˜ì§‘ ì„¤ì • (ELK Stack ë˜ëŠ” CloudWatch)

#### ë°±ì—…
- [ ] PostgreSQL ìë™ ë°±ì—… ì„¤ì •
- [ ] ë°±ì—… ë³µì› í…ŒìŠ¤íŠ¸
- [ ] Redis ìŠ¤ëƒ…ìƒ· ì„¤ì •

### 9.3 ë°°í¬ ëª…ë ¹ì–´

```bash
# 1. í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
cp app/.env.example app/.env
nano app/.env  # í™˜ê²½ ë³€ìˆ˜ í¸ì§‘

# 2. Docker ë¹Œë“œ ë° ì‹¤í–‰
docker-compose build
docker-compose up -d

# 3. ë°ì´í„°ë² ì´ìŠ¤ ë§ˆì´ê·¸ë ˆì´ì…˜
docker-compose exec app alembic upgrade head

# 4. í—¬ìŠ¤ ì²´í¬
curl http://localhost:8000/health

# 5. ë¡œê·¸ í™•ì¸
docker-compose logs -f app
docker-compose logs -f celery_worker

# 6. ì„œë¹„ìŠ¤ ì¬ì‹œì‘
docker-compose restart app

# 7. ìŠ¤ì¼€ì¼ë§ (Worker ì¦ì„¤)
docker-compose up -d --scale celery_worker=3
```

### 9.4 í—¬ìŠ¤ ì²´í¬

**ì—”ë“œí¬ì¸íŠ¸**: `GET /health`

**ì‘ë‹µ ì˜ˆì‹œ**:
```json
{
  "status": "healthy",
  "timestamp": "2025-11-03T12:00:00Z",
  "database": "connected",
  "redis": "connected",
  "celery": {
    "workers": 2,
    "active_tasks": 5
  }
}
```

### 9.5 ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ

#### Grafana ì£¼ìš” íŒ¨ë„
1. **HTTP Requests** (QPS)
2. **Response Time** (P50, P95, P99)
3. **Error Rate** (4xx, 5xx)
4. **Active Crawl Tasks**
5. **Crawl Success Rate**
6. **Database Connection Pool**
7. **Redis Memory Usage**
8. **Celery Queue Length**

---

## 10. ê°œë°œ ê°€ì´ë“œ

### 10.1 ìƒˆë¡œìš´ í¬ë¡¤ëŸ¬ ì¶”ê°€

#### Step 1: í¬ë¡¤ëŸ¬ í´ë˜ìŠ¤ ì‘ì„±

```python
# college_crawlers.py

class NewUniversityCrawler(BaseCrawler):
    """ìƒˆ ëŒ€í•™êµ í¬ë¡¤ëŸ¬"""

    BASE_URL = "https://www.newuniv.ac.kr"
    NOTICE_URL = f"{BASE_URL}/notice"

    async def crawl(self, url: str) -> List[Dict]:
        """í¬ë¡¤ë§ ì‹¤í–‰"""

        # HTML ê°€ì ¸ì˜¤ê¸° (ë™ì  í˜ì´ì§€ë©´ use_browser=True)
        html = await self.fetch_html(url, use_browser=False)

        # íŒŒì‹±
        soup = BeautifulSoup(html, 'lxml')

        notices = []
        for item in soup.select('.notice-item'):  # ì…€ë ‰í„° ìˆ˜ì • í•„ìš”
            notice = {
                "title": item.select_one('.title').text.strip(),
                "url": urljoin(self.BASE_URL, item.select_one('a')['href']),
                "date": item.select_one('.date').text.strip(),
                "category": self._extract_category(item),
                "source": "newuniv",
                "fingerprint": self._generate_fingerprint(item['href'])
            }
            notices.append(notice)

        return notices

    def _extract_category(self, item) -> str:
        """ì¹´í…Œê³ ë¦¬ ì¶”ì¶œ (ëŒ€í•™ë§ˆë‹¤ ë‹¤ë¦„)"""
        # êµ¬í˜„ í•„ìš”
        pass
```

#### Step 2: í¬ë¡¤ëŸ¬ ë“±ë¡

```python
# college_crawlers.py í•˜ë‹¨

COLLEGE_CRAWLERS = {
    "konkuk": KonkukCrawler,
    "seoultech": SeoulTechCrawler,
    "newuniv": NewUniversityCrawler,  # ì¶”ê°€
}
```

#### Step 3: í…ŒìŠ¤íŠ¸ ì‘ì„±

```python
# tests/unit/test_crawlers.py

async def test_newuniv_crawler():
    """ìƒˆ ëŒ€í•™ í¬ë¡¤ëŸ¬ í…ŒìŠ¤íŠ¸"""
    crawler = NewUniversityCrawler()

    results = await crawler.crawl("https://www.newuniv.ac.kr/notice")

    assert len(results) > 0
    assert all("title" in r for r in results)
    assert all("url" in r for r in results)
```

### 10.2 ìƒˆë¡œìš´ API ì—”ë“œí¬ì¸íŠ¸ ì¶”ê°€

#### Step 1: Pydantic ìŠ¤í‚¤ë§ˆ ì •ì˜

```python
# schemas.py

class CustomReportRequest(BaseModel):
    """ì»¤ìŠ¤í…€ ë¦¬í¬íŠ¸ ìš”ì²­"""
    start_date: datetime
    end_date: datetime
    colleges: List[str]

class CustomReportResponse(BaseModel):
    """ì»¤ìŠ¤í…€ ë¦¬í¬íŠ¸ ì‘ë‹µ"""
    total_notices: int
    by_college: Dict[str, int]
    by_category: Dict[str, int]
```

#### Step 2: CRUD í•¨ìˆ˜ ì¶”ê°€

```python
# crud.py

def generate_custom_report(
    db: Session,
    start_date: datetime,
    end_date: datetime,
    colleges: List[str]
) -> Dict[str, Any]:
    """ì»¤ìŠ¤í…€ ë¦¬í¬íŠ¸ ìƒì„±"""

    query = db.query(CrawlNotice).filter(
        CrawlNotice.created_at >= start_date,
        CrawlNotice.created_at <= end_date,
        CrawlNotice.source.in_(colleges)
    )

    notices = query.all()

    # ì§‘ê³„
    by_college = {}
    by_category = {}

    for notice in notices:
        by_college[notice.source] = by_college.get(notice.source, 0) + 1
        by_category[notice.category] = by_category.get(notice.category, 0) + 1

    return {
        "total_notices": len(notices),
        "by_college": by_college,
        "by_category": by_category
    }
```

#### Step 3: API ë¼ìš°í„° ì¶”ê°€

```python
# api.py

@router.post("/reports/custom", response_model=CustomReportResponse)
async def generate_report(
    request: CustomReportRequest,
    db: Session = Depends(get_db),
    api_key: str = Depends(verify_api_key)
):
    """ì»¤ìŠ¤í…€ ë¦¬í¬íŠ¸ ìƒì„±"""

    report = crud.generate_custom_report(
        db,
        request.start_date,
        request.end_date,
        request.colleges
    )

    return report
```

### 10.3 ì½”ë“œ ìŠ¤íƒ€ì¼ ê°€ì´ë“œ

#### Python ìŠ¤íƒ€ì¼
```bash
# ì½”ë“œ í¬ë§·íŒ…
black app/ --line-length 100

# Import ì •ë ¬
isort app/

# ë¦°íŒ…
flake8 app/ --max-line-length 100

# íƒ€ì… ì²´í¬
mypy app/ --ignore-missing-imports
```

#### Commit ë©”ì‹œì§€
```
feat: ìƒˆë¡œìš´ ê¸°ëŠ¥ ì¶”ê°€
fix: ë²„ê·¸ ìˆ˜ì •
docs: ë¬¸ì„œ ìˆ˜ì •
refactor: ë¦¬íŒ©í† ë§
test: í…ŒìŠ¤íŠ¸ ì¶”ê°€
chore: ë¹Œë“œ/ì„¤ì • ë³€ê²½
```

### 10.4 íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

#### ë¬¸ì œ 1: PostgreSQL ì—°ê²° ì˜¤ë¥˜
```bash
# ì¦ìƒ
sqlalchemy.exc.OperationalError: could not connect to server

# í•´ê²°
1. PostgreSQL ì„œë¹„ìŠ¤ í™•ì¸
   docker-compose ps postgres

2. ì—°ê²° ì •ë³´ í™•ì¸
   echo $DATABASE_URL

3. ë„¤íŠ¸ì›Œí¬ í™•ì¸
   docker-compose exec app ping postgres

4. ì¬ì‹œì‘
   docker-compose restart postgres
```

#### ë¬¸ì œ 2: Celery ì‘ì—… ì‹¤í–‰ ì•ˆë¨
```bash
# ì¦ìƒ
ì‘ì—…ì´ Redis íì—ë§Œ ìŒ“ì´ê³  ì‹¤í–‰ ì•ˆë¨

# í•´ê²°
1. Worker ìƒíƒœ í™•ì¸
   docker-compose ps celery_worker

2. Worker ë¡œê·¸ í™•ì¸
   docker-compose logs celery_worker

3. Redis ì—°ê²° í™•ì¸
   docker-compose exec redis redis-cli ping

4. Worker ì¬ì‹œì‘
   docker-compose restart celery_worker
```

#### ë¬¸ì œ 3: Rate Limit 429 ì˜¤ë¥˜
```bash
# ì¦ìƒ
{"error": "Rate limit exceeded"}

# í•´ê²°
1. Redisì—ì„œ Rate Limit í‚¤ í™•ì¸
   docker-compose exec redis redis-cli KEYS "rate_limit:*"

2. íŠ¹ì • í´ë¼ì´ì–¸íŠ¸ ë¦¬ì…‹
   docker-compose exec redis redis-cli DEL "rate_limit:api_key:YOUR_KEY"

3. ì„¤ì • ì¡°ì • (.env)
   MAX_REQUESTS_PER_MINUTE=120
```

---

## ë¶€ë¡

### A. API ì—”ë“œí¬ì¸íŠ¸ ì „ì²´ ëª©ë¡

| ì—”ë“œí¬ì¸íŠ¸ | ë©”ì„œë“œ | ì„¤ëª… | ì¸ì¦ | í˜ì´ì§• |
|-----------|--------|------|------|--------|
| `/health` | GET | í—¬ìŠ¤ ì²´í¬ | âŒ | âŒ |
| `/metrics` | GET | Prometheus ë©”íŠ¸ë¦­ | âŒ | âŒ |
| `/jobs` | GET | Job ëª©ë¡ ì¡°íšŒ | âœ… | âœ… |
| `/jobs` | POST | Job ìƒì„± | âœ… | âŒ |
| `/jobs/{id}` | GET | Job ì¡°íšŒ | âœ… | âŒ |
| `/jobs/{id}` | DELETE | Job ì‚­ì œ | âœ… | âŒ |
| `/jobs/{id}/pause` | POST | Job ì¼ì‹œì •ì§€ | âœ… | âŒ |
| `/jobs/{id}/resume` | POST | Job ì¬ê°œ | âœ… | âŒ |
| `/jobs/{id}/tasks` | GET | Jobì˜ Task ëª©ë¡ | âœ… | âœ… |
| `/jobs/{id}/statistics` | GET | Job í†µê³„ | âœ… | âŒ |
| `/tasks` | GET | Task ëª©ë¡ | âœ… | âœ… |
| `/tasks/{id}` | GET | Task ì¡°íšŒ | âœ… | âŒ |
| `/documents` | GET | ê³µì§€ì‚¬í•­ ëª©ë¡ | âœ… | âœ… |
| `/documents/{id}` | GET | ê³µì§€ì‚¬í•­ ì¡°íšŒ | âœ… | âŒ |
| `/documents/search` | GET | ê³µì§€ì‚¬í•­ ê²€ìƒ‰ | âœ… | âœ… |
| `/run-crawler/{name}` | POST | í¬ë¡¤ëŸ¬ ìˆ˜ë™ ì‹¤í–‰ | âœ… | âŒ |

### B. ë°ì´í„°ë² ì´ìŠ¤ ìŠ¤í‚¤ë§ˆ

```sql
-- crawl_job í…Œì´ë¸”
CREATE TABLE crawl_job (
    id SERIAL PRIMARY KEY,
    name VARCHAR(128) NOT NULL UNIQUE,
    priority VARCHAR(4) NOT NULL,
    schedule_cron VARCHAR(64),
    seed_type VARCHAR(20) NOT NULL,
    seed_payload JSONB NOT NULL,
    render_mode VARCHAR(20) NOT NULL,
    rate_limit_per_host FLOAT DEFAULT 1.0,
    max_depth INTEGER DEFAULT 1,
    robots_policy VARCHAR(20) NOT NULL,
    status VARCHAR(20) DEFAULT 'ACTIVE',
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);

CREATE INDEX idx_job_name ON crawl_job(name);
CREATE INDEX idx_job_status ON crawl_job(status);

-- crawl_task í…Œì´ë¸”
CREATE TABLE crawl_task (
    id SERIAL PRIMARY KEY,
    job_id INTEGER NOT NULL REFERENCES crawl_job(id) ON DELETE CASCADE,
    url TEXT NOT NULL,
    status VARCHAR(20) DEFAULT 'PENDING',
    retries INTEGER DEFAULT 0,
    last_error TEXT,
    started_at TIMESTAMP WITH TIME ZONE,
    finished_at TIMESTAMP WITH TIME ZONE,
    http_status INTEGER,
    content_hash VARCHAR(64),
    blocked_flag BOOLEAN DEFAULT FALSE,
    cost_ms_browser INTEGER
);

CREATE INDEX idx_task_job_id ON crawl_task(job_id);
CREATE INDEX idx_task_status ON crawl_task(status);
CREATE INDEX idx_task_url ON crawl_task(url);

-- crawl_notice í…Œì´ë¸”
CREATE TABLE crawl_notice (
    id SERIAL PRIMARY KEY,
    job_id INTEGER NOT NULL REFERENCES crawl_job(id) ON DELETE CASCADE,
    url TEXT NOT NULL,
    title TEXT,
    writer VARCHAR(128),
    date VARCHAR(32),
    category VARCHAR(64),
    source VARCHAR(64),
    extracted JSONB,
    raw TEXT,
    fingerprint VARCHAR(64) NOT NULL UNIQUE,
    snapshot_version VARCHAR(32),
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);

CREATE INDEX idx_notice_job_id ON crawl_notice(job_id);
CREATE INDEX idx_notice_url ON crawl_notice(url);
CREATE INDEX idx_notice_category ON crawl_notice(category);
CREATE INDEX idx_notice_fingerprint ON crawl_notice(fingerprint);
CREATE INDEX idx_notice_created_at ON crawl_notice(created_at);
```

### C. Celery Beat ìŠ¤ì¼€ì¤„ ì˜ˆì‹œ

```python
from celery.schedules import crontab

celery_app.conf.beat_schedule = {
    # ë§¤ì¼ ì˜¤ì „ 6ì‹œ ì „ì²´ í¬ë¡¤ë§
    'daily-morning-crawl': {
        'task': 'tasks.crawl_all_universities',
        'schedule': crontab(hour=6, minute=0),
    },

    # 6ì‹œê°„ë§ˆë‹¤ ì¥í•™ê¸ˆ ê³µì§€ í¬ë¡¤ë§
    'scholarship-crawl': {
        'task': 'tasks.crawl_scholarship_notices',
        'schedule': crontab(hour='*/6'),
    },

    # ë§¤ì‹œê°„ í—¬ìŠ¤ ì²´í¬
    'hourly-health-check': {
        'task': 'tasks.health_check',
        'schedule': crontab(minute=0),
    },

    # ë§¤ì¼ ìì • í†µê³„ ì§‘ê³„
    'daily-statistics': {
        'task': 'tasks.generate_daily_statistics',
        'schedule': crontab(hour=0, minute=0),
    }
}
```

---

## ë³€ê²½ ì´ë ¥

| ë²„ì „ | ë‚ ì§œ | ë³€ê²½ ë‚´ìš© |
|------|------|-----------|
| 1.0 | 2025-11-03 | ìµœì´ˆ ì‘ì„± |

---

**ë¬¸ì˜**: [GitHub Issues](https://github.com/your-repo/issues)
