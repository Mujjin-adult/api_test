# í¬ë¡¤ë§ ì„œë²„ ì•„í‚¤í…ì²˜ ë° ì‘ë™ ì›ë¦¬

## ğŸ“ ì „ì²´ ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    í¬ë¡¤ë§ ì„œë²„ (FastAPI)                        â”‚
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚  â”‚  FastAPI     â”‚  â”‚  Celery      â”‚  â”‚  Auto        â”‚        â”‚
â”‚  â”‚  Web Server  â”‚  â”‚  Worker      â”‚  â”‚  Scheduler   â”‚        â”‚
â”‚  â”‚  (main.py)   â”‚  â”‚  (tasks.py)  â”‚  â”‚  (auto_      â”‚        â”‚
â”‚  â”‚              â”‚  â”‚              â”‚  â”‚  scheduler)  â”‚        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â”‚         â”‚                  â”‚                  â”‚                 â”‚
â”‚         â”‚                  â”‚                  â”‚                 â”‚
â”‚         â–¼                  â–¼                  â–¼                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚             College Crawler (college_crawlers.py)     â”‚     â”‚
â”‚  â”‚  - crawl_volunteer()                                  â”‚     â”‚
â”‚  â”‚  - crawl_job()                                        â”‚     â”‚
â”‚  â”‚  - crawl_scholarship()                                â”‚     â”‚
â”‚  â”‚  - crawl_all()                                        â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚  â”‚  Circuit     â”‚  â”‚  Rate        â”‚  â”‚  Robots.txt  â”‚        â”‚
â”‚  â”‚  Breaker     â”‚  â”‚  Limiter     â”‚  â”‚  Parser      â”‚        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                  â”‚                  â”‚
         â–¼                  â–¼                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   ì™¸ë¶€ ì‹œìŠ¤í…œ                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚  â”‚ PostgreSQL   â”‚  â”‚ Redis        â”‚  â”‚ ì¸ì²œëŒ€ ì›¹ì‚¬ì´íŠ¸â”‚         â”‚
â”‚  â”‚ (crawl_      â”‚  â”‚ (Celery      â”‚  â”‚ (www.inu.    â”‚         â”‚
â”‚  â”‚ notice,      â”‚  â”‚ Broker/      â”‚  â”‚ ac.kr)       â”‚         â”‚
â”‚  â”‚ crawl_job)   â”‚  â”‚ Result)      â”‚  â”‚              â”‚         â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”„ í¬ë¡¤ë§ ì„œë²„ ì‘ë™ íë¦„

### 1ï¸âƒ£ ì‹œìŠ¤í…œ ì‹œì‘ (Startup)

**main.pyì˜ lifespan í•¨ìˆ˜**

```python
@asynccontextmanager
async def lifespan(app: FastAPI):
    # 1. ì‹œì‘ ì‹œ ì´ˆê¸°í™”
    print("Starting College Notice Crawler...")

    # 2. ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”
    init_database()  # í…Œì´ë¸” ìƒì„±, ì—°ê²° í™•ì¸

    # 3. ìŠ¤ì¼€ì¤„ëŸ¬ ì´ˆê¸°í™”
    init_college_scheduler()
        â†“
        â”œâ”€ CollegeAutoScheduler ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
        â”œâ”€ register_all_jobs() - í¬ë¡¤ë§ ì‘ì—…ì„ DBì— ë“±ë¡
        â””â”€ update_celery_schedule() - Celery Beat ìŠ¤ì¼€ì¤„ ì—…ë°ì´íŠ¸

    yield  # ì• í”Œë¦¬ì¼€ì´ì…˜ ì‹¤í–‰

    # ì¢…ë£Œ ì‹œ
    print("Shutting down...")
```

**ì´ˆê¸°í™” ê³¼ì •:**
1. FastAPI ì„œë²„ ì‹œì‘
2. ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ë° í…Œì´ë¸” ìƒì„±
3. 8ê°œ í¬ë¡¤ë§ ì‘ì—…ì„ `crawl_job` í…Œì´ë¸”ì— ë“±ë¡
4. Celery Beatì— ìŠ¤ì¼€ì¤„ ë“±ë¡

---

### 2ï¸âƒ£ ìë™ ìŠ¤ì¼€ì¤„ëŸ¬ (Auto Scheduler)

**auto_scheduler.pyì˜ CollegeAutoScheduler í´ë˜ìŠ¤**

#### ì‘ì—… ë“±ë¡ (register_all_jobs)

```python
job_configs = [
    {
        "name": "ë´‰ì‚¬ ê³µì§€ì‚¬í•­ í¬ë¡¤ë§",
        "priority": "P1",
        "seed_payload": {
            "urls": ["https://www.inu.ac.kr/bbs/inu/253/artclList.do"],
            "category": "volunteer",
            "page_num": "253"
        },
        "schedule_cron": "0 */2 * * *",  # 2ì‹œê°„ë§ˆë‹¤
        "rate_limit_per_host": 0.5,      # 2ì´ˆì— 1íšŒ
        "max_depth": 1
    },
    # ... 7ê°œ ì‘ì—… ë” (ì·¨ì—…, ì¥í•™ê¸ˆ, ì¼ë°˜í–‰ì‚¬, êµìœ¡ì‹œí—˜, ë“±ë¡ê¸ˆë‚©ë¶€, í•™ì , í•™ìœ„)
]
```

**ë“±ë¡ ê³¼ì •:**
```
1. job_configs ë¦¬ìŠ¤íŠ¸ ìˆœíšŒ
   â†“
2. ê° ì‘ì—…ì„ DBì— ë“±ë¡ (ì¤‘ë³µ ì²´í¬)
   â†“
3. crawl_job í…Œì´ë¸”ì— ì €ì¥
   - name: "ë´‰ì‚¬ ê³µì§€ì‚¬í•­ í¬ë¡¤ë§"
   - status: ACTIVE
   - schedule_cron: "0 */2 * * *"
   - seed_payload: JSON í˜•íƒœë¡œ ì €ì¥
```

#### ìŠ¤ì¼€ì¤„ ì—…ë°ì´íŠ¸ (update_celery_schedule)

```python
for config in job_configs:
    schedule_name = "college-ë´‰ì‚¬-ê³µì§€ì‚¬í•­-í¬ë¡¤ë§"

    # crontab ë¬¸ìì—´ íŒŒì‹±: "0 */2 * * *"
    # â†’ minute=0, hour=*/2, day=*, month=*, day_of_week=*

    celery_app.conf.beat_schedule[schedule_name] = {
        "task": "tasks.college_crawl_task",  # ì‹¤í–‰í•  íƒœìŠ¤í¬
        "schedule": crontab(minute=0, hour="*/2"),  # 2ì‹œê°„ë§ˆë‹¤
        "args": ("ë´‰ì‚¬ ê³µì§€ì‚¬í•­ í¬ë¡¤ë§",),  # job_name
        "options": {"priority": 1}  # P1 = ìš°ì„ ìˆœìœ„ 1
    }
```

**ìŠ¤ì¼€ì¤„ ë“±ë¡ ê²°ê³¼:**
```
Celery Beat ìŠ¤ì¼€ì¤„:
â”œâ”€ college-ë´‰ì‚¬-ê³µì§€ì‚¬í•­-í¬ë¡¤ë§    â†’ 2ì‹œê°„ë§ˆë‹¤ ì‹¤í–‰ (0 */2 * * *)
â”œâ”€ college-ì·¨ì—…-ê³µì§€ì‚¬í•­-í¬ë¡¤ë§    â†’ 3ì‹œê°„ë§ˆë‹¤ ì‹¤í–‰ (0 */3 * * *)
â”œâ”€ college-ì¥í•™ê¸ˆ-ê³µì§€ì‚¬í•­-í¬ë¡¤ë§  â†’ 4ì‹œê°„ë§ˆë‹¤ ì‹¤í–‰ (0 */4 * * *)
â”œâ”€ college-ì¼ë°˜í–‰ì‚¬/ì±„ìš©-í¬ë¡¤ë§    â†’ 6ì‹œê°„ë§ˆë‹¤ ì‹¤í–‰ (0 */6 * * *)
â”œâ”€ college-êµìœ¡ì‹œí—˜-í¬ë¡¤ë§         â†’ 6ì‹œê°„ë§ˆë‹¤ ì‹¤í–‰ (0 */6 * * *)
â”œâ”€ college-ë“±ë¡ê¸ˆë‚©ë¶€-í¬ë¡¤ë§       â†’ 8ì‹œê°„ë§ˆë‹¤ ì‹¤í–‰ (0 */8 * * *)
â”œâ”€ college-í•™ì -í¬ë¡¤ë§             â†’ 8ì‹œê°„ë§ˆë‹¤ ì‹¤í–‰ (0 */8 * * *)
â””â”€ college-í•™ìœ„-í¬ë¡¤ë§             â†’ 8ì‹œê°„ë§ˆë‹¤ ì‹¤í–‰ (0 */8 * * *)
```

---

### 3ï¸âƒ£ Celery ì‘ì—… í ì‹œìŠ¤í…œ

#### Celery ì•„í‚¤í…ì²˜

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Celery Beat                          â”‚
â”‚  (ìŠ¤ì¼€ì¤„ëŸ¬ - ì •í•´ì§„ ì‹œê°„ì— íƒœìŠ¤í¬ íŠ¸ë¦¬ê±°)                â”‚
â”‚                                                          â”‚
â”‚  ë§¤ 2ì‹œê°„ë§ˆë‹¤:                                           â”‚
â”‚  college_crawl_task.delay("ë´‰ì‚¬ ê³µì§€ì‚¬í•­ í¬ë¡¤ë§")       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  Redis (Broker)                         â”‚
â”‚  - íƒœìŠ¤í¬ í ì €ì¥                                        â”‚
â”‚  - ë©”ì‹œì§€ ë¸Œë¡œì»¤ ì—­í•                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Celery Worker (tasks.py)                   â”‚
â”‚  - íì—ì„œ íƒœìŠ¤í¬ ê°€ì ¸ì˜¤ê¸°                                â”‚
â”‚  - college_crawl_task ì‹¤í–‰                              â”‚
â”‚  - ê²°ê³¼ë¥¼ Redisì— ì €ì¥                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          Redis (Result Backend)                         â”‚
â”‚  - ì‘ì—… ê²°ê³¼ ì €ì¥                                        â”‚
â”‚  - ì„±ê³µ/ì‹¤íŒ¨ ìƒíƒœ                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### Celery ì„¤ì • (tasks.py)

```python
celery_app = Celery(
    "school_notices",
    broker=REDIS_URL,           # Redis (ë©”ì‹œì§€ í)
    backend=REDIS_URL,          # Redis (ê²°ê³¼ ì €ì¥)
)

celery_app.conf.update(
    task_serializer="json",
    accept_content=["json"],
    result_serializer="json",
    timezone="Asia/Seoul",
    enable_utc=True,
    task_track_started=True,
    task_time_limit=30 * 60,        # 30ë¶„ ìµœëŒ€ ì‹¤í–‰ ì‹œê°„
    task_soft_time_limit=25 * 60,   # 25ë¶„ ì†Œí”„íŠ¸ íƒ€ì„ì•„ì›ƒ
    worker_prefetch_multiplier=1,   # í•œ ë²ˆì— 1ê°œ íƒœìŠ¤í¬ë§Œ ê°€ì ¸ì˜´
    worker_max_tasks_per_child=1000 # 1000ê°œ ì‘ì—… í›„ ì›Œì»¤ ì¬ì‹œì‘
)
```

---

### 4ï¸âƒ£ í¬ë¡¤ë§ íƒœìŠ¤í¬ ì‹¤í–‰ íë¦„

#### college_crawl_task (tasks.py)

**ë‹¨ê³„ë³„ ì‹¤í–‰ íë¦„:**

```python
@celery_app.task(bind=True, max_retries=3, default_retry_delay=30)
def college_crawl_task(self, job_name: str):
    """ëŒ€í•™ ê³µì§€ì‚¬í•­ í¬ë¡¤ë§ íƒœìŠ¤í¬"""

    # 1ë‹¨ê³„: ì´ˆê¸°í™”
    request_id = uuid.uuid4()
    start_time = time.time()

    # 2ë‹¨ê³„: í¬ë¡¤ë§ ì‹¤í–‰ (job_nameì— ë”°ë¼ ë¶„ê¸°)
    if job_name == "ë´‰ì‚¬ ê³µì§€ì‚¬í•­ í¬ë¡¤ë§":
        results = college_crawler.crawl_volunteer()
    elif job_name == "ì·¨ì—… ê³µì§€ì‚¬í•­ í¬ë¡¤ë§":
        results = college_crawler.crawl_job()
    elif job_name == "ì¥í•™ê¸ˆ ê³µì§€ì‚¬í•­ í¬ë¡¤ë§":
        results = college_crawler.crawl_scholarship()
    else:
        results = college_crawler.crawl_all()

    # 3ë‹¨ê³„: ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥
    #   â”œâ”€ ì¤‘ë³µ URL ì²´í¬ (ë²Œí¬ ì¡°íšŒ)
    #   â”œâ”€ ìƒˆ ë¬¸ì„œë§Œ í•„í„°ë§
    #   â”œâ”€ fingerprint ìƒì„± (SHA-256)
    #   â””â”€ ë²Œí¬ ì‚½ì… (bulk_create_documents)

    # 4ë‹¨ê³„: ì„±ëŠ¥ ë¡œê¹…
    duration = time.time() - start_time
    log_performance(
        "college_crawl_task",
        duration,
        {"job_name": job_name, "total": 50, "saved": 10}
    )

    # 5ë‹¨ê³„: ê²°ê³¼ ë°˜í™˜
    return {
        "status": "success",
        "job_name": job_name,
        "total_items": 50,
        "saved_items": 10,
        "skipped_items": 40,
        "duration": 15.2
    }
```

**ì¬ì‹œë„ ë¡œì§:**

```python
# ì˜ˆì™¸ ë°œìƒ ì‹œ
except Exception as exc:
    # ì§€ìˆ˜ ë°±ì˜¤í”„ë¡œ ì¬ì‹œë„
    # 1ì°¨: 7ì´ˆ í›„ (2^0 + 5 + random)
    # 2ì°¨: 9ì´ˆ í›„ (2^1 + 5 + random)
    # 3ì°¨: 13ì´ˆ í›„ (2^2 + 5 + random)
    self.retry(exc=exc, countdown=(2**self.request.retries) + 5)
```

---

### 5ï¸âƒ£ ì‹¤ì œ í¬ë¡¤ë§ ì‹¤í–‰ (College Crawler)

#### CollegeCrawler í´ë˜ìŠ¤ (college_crawlers.py)

**ì£¼ìš” ê¸°ëŠ¥:**
1. HTTP ìš”ì²­ (Circuit Breaker, Rate Limiter ì ìš©)
2. HTML íŒŒì‹± (BeautifulSoup)
3. ë°ì´í„° ì¶”ì¶œ
4. ì—ëŸ¬ ì²˜ë¦¬

#### crawl_volunteer() ë©”ì„œë“œ ì˜ˆì‹œ

```python
def crawl_volunteer(self) -> List[Dict[str, Any]]:
    """ë´‰ì‚¬ ê³µì§€ì‚¬í•­ í¬ë¡¤ë§"""

    url = "https://www.inu.ac.kr/bbs/inu/253/artclList.do"
    page_num = "253"
    source = "volunteer"
    all_notices = []

    # ìµœëŒ€ 5í˜ì´ì§€ê¹Œì§€ í¬ë¡¤ë§
    for page in range(1, self.max_pages + 1):
        try:
            # 1ë‹¨ê³„: HTTP ìš”ì²­ (ì¬ì‹œë„ ë¡œì§ í¬í•¨)
            payload = {
                "page": str(page),
                "srchBbsNttCd": page_num
            }
            response = self._make_request_with_retry(url, payload)

            # 2ë‹¨ê³„: HTML íŒŒì‹±
            soup = BeautifulSoup(response.text, 'html.parser')
            rows = soup.select('table.board-table tbody tr')

            # 3ë‹¨ê³„: ê° í–‰ íŒŒì‹±
            for row in rows:
                try:
                    notice = self._parse_notice_row(row, source)
                    if notice:
                        all_notices.append(notice)
                except Exception as e:
                    logger.error(f"Failed to parse row: {e}")
                    continue

            # 4ë‹¨ê³„: í˜ì´ì§€ ê°„ ë”œë ˆì´ (Rate Limiting)
            time.sleep(random.uniform(1, 3))

        except TemporaryError as e:
            # ì¼ì‹œì  ì—ëŸ¬: ë¡œê¹…ë§Œ í•˜ê³  ë‹¤ìŒ í˜ì´ì§€ë¡œ
            logger.warning(f"Temporary error on page {page}: {e}")
            break

        except PermanentError as e:
            # ì˜êµ¬ì  ì—ëŸ¬: ì¦‰ì‹œ ì¤‘ë‹¨
            logger.error(f"Permanent error on page {page}: {e}")
            break

    return all_notices
```

#### _make_request_with_retry ë©”ì„œë“œ

**Circuit Breaker íŒ¨í„´ ì ìš©:**

```python
def _make_request_with_retry(self, url: str, payload: dict, retry_count: int = 0):
    try:
        # Circuit Breakerë¡œ ë³´í˜¸ëœ ìš”ì²­
        def make_request():
            response = self.session.post(url, data=payload, timeout=30)

            # HTTP ìƒíƒœ ì½”ë“œ ë¶„ë¥˜
            if response.status_code == 429:
                raise TemporaryError("Rate limited")
            elif response.status_code in [500, 502, 503, 504]:
                raise TemporaryError("Server error")
            elif response.status_code in [400, 401, 403, 404]:
                raise PermanentError("Client error")

            return response

        # Circuit Breaker ì‹¤í–‰
        return self.circuit_breaker.call(make_request)

    except TemporaryError as e:
        # ì¬ì‹œë„ (ìµœëŒ€ 3íšŒ)
        if retry_count < self.max_retries:
            wait_time = (2 ** retry_count) + random.uniform(1, 3)
            time.sleep(wait_time)
            return self._make_request_with_retry(url, payload, retry_count + 1)
        else:
            raise
```

**Circuit Breaker ìƒíƒœ:**
```
CLOSED (ì •ìƒ)
   â†“ (5ë²ˆ ì‹¤íŒ¨)
OPEN (ì°¨ë‹¨)
   â†“ (60ì´ˆ ëŒ€ê¸°)
HALF_OPEN (í…ŒìŠ¤íŠ¸)
   â†“ (2ë²ˆ ì„±ê³µ)
CLOSED (ë³µêµ¬)
```

---

### 6ï¸âƒ£ ë°ì´í„° ì €ì¥ íë¦„

#### ë²Œí¬ ì‚½ì… ìµœì í™”

**Before (ê°œë³„ ì‚½ì…):**
```python
for item in results:
    create_document(db, item)  # 50ê°œ â†’ 50ë²ˆ DB ì¿¼ë¦¬
```

**After (ë²Œí¬ ì‚½ì…):**
```python
# 1ë‹¨ê³„: ì¤‘ë³µ URL ì²´í¬ (1ë²ˆ ì¿¼ë¦¬)
urls_to_check = [item['url'] for item in results]
existing_docs = db.query(CrawlNotice).filter(
    CrawlNotice.url.in_(urls_to_check)
).all()
existing_urls = {doc.url for doc in existing_docs}

# 2ë‹¨ê³„: ìƒˆ ë¬¸ì„œë§Œ í•„í„°ë§
docs_to_insert = []
for item in results:
    if item['url'] not in existing_urls:
        docs_to_insert.append({
            "url": item['url'],
            "title": item['title'],
            "fingerprint": hashlib.sha256(...).hexdigest(),
            ...
        })

# 3ë‹¨ê³„: ë²Œí¬ ì‚½ì… (1ë²ˆ ì¿¼ë¦¬)
db.bulk_insert_mappings(CrawlNotice, docs_to_insert)
db.commit()
```

**ì„±ëŠ¥ ê°œì„ :**
- 50ê°œ ë¬¸ì„œ ì‚½ì…: 50ë²ˆ ì¿¼ë¦¬ â†’ 2ë²ˆ ì¿¼ë¦¬ (25ë°° ê°œì„ )

---

### 7ï¸âƒ£ ì•ˆì •ì„± ë³´ì¥ ë©”ì»¤ë‹ˆì¦˜

#### Circuit Breaker

**ëª©ì :** ì—°ì†ëœ ì‹¤íŒ¨ ì‹œ ì‹œìŠ¤í…œ ë³´í˜¸

```python
circuit_breaker = CircuitBreaker(
    name="inu_crawler",
    failure_threshold=5,      # 5ë²ˆ ì‹¤íŒ¨ ì‹œ ì°¨ë‹¨
    success_threshold=2,      # 2ë²ˆ ì„±ê³µ ì‹œ ë³µêµ¬
    timeout=60.0              # 60ì´ˆ ëŒ€ê¸°
)

# CLOSED â†’ OPEN â†’ HALF_OPEN â†’ CLOSED
```

#### Rate Limiter

**ëª©ì :** ì„œë²„ ë¶€í•˜ ë°©ì§€

```python
rate_limiter = RateLimiter(
    requests_per_minute=60  # ë¶„ë‹¹ ìµœëŒ€ 60íšŒ ìš”ì²­
)

# ì‚¬ìš©
if not rate_limiter.can_make_request(url):
    rate_limiter.wait_for_request(url)
```

#### Robots.txt ì¤€ìˆ˜

```python
robots_manager = RobotsManager()

# í¬ë¡¤ë§ í—ˆìš© í™•ì¸
if robots_manager.is_allowed(url, user_agent):
    # crawl-delay ì¤€ìˆ˜
    robots_manager.wait_if_needed(url, user_agent)
    # í¬ë¡¤ë§ ì‹¤í–‰
```

---

## ğŸ”„ ì „ì²´ ì‹¤í–‰ íë¦„ (ì—”ë“œ íˆ¬ ì—”ë“œ)

### ìë™ ì‹¤í–‰ (ìŠ¤ì¼€ì¤„ëŸ¬)

```
1. Celery Beat ì‹œì‘
   â””â”€ ìŠ¤ì¼€ì¤„ì— ë”°ë¼ íƒœìŠ¤í¬ íŠ¸ë¦¬ê±°
      â”œâ”€ 00:00 - ë´‰ì‚¬ í¬ë¡¤ë§ ì‹œì‘
      â”œâ”€ 02:00 - ë´‰ì‚¬ í¬ë¡¤ë§ ì‹œì‘
      â”œâ”€ 03:00 - ì·¨ì—… í¬ë¡¤ë§ ì‹œì‘
      â”œâ”€ 04:00 - ë´‰ì‚¬, ì¥í•™ê¸ˆ í¬ë¡¤ë§ ì‹œì‘
      â””â”€ ...

2. college_crawl_task.delay("ë´‰ì‚¬ ê³µì§€ì‚¬í•­ í¬ë¡¤ë§")
   â””â”€ Redis íì— íƒœìŠ¤í¬ ì¶”ê°€

3. Celery Worker
   â””â”€ íì—ì„œ íƒœìŠ¤í¬ ê°€ì ¸ì˜¤ê¸°
      â”œâ”€ college_crawl_task ì‹¤í–‰
      â”œâ”€ college_crawler.crawl_volunteer() í˜¸ì¶œ
      â””â”€ ê²°ê³¼ë¥¼ Redisì— ì €ì¥

4. college_crawler.crawl_volunteer()
   â””â”€ ì‹¤ì œ í¬ë¡¤ë§ ìˆ˜í–‰
      â”œâ”€ HTTP ìš”ì²­ (Circuit Breaker, Rate Limiter)
      â”œâ”€ HTML íŒŒì‹± (BeautifulSoup)
      â”œâ”€ ë°ì´í„° ì¶”ì¶œ
      â””â”€ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜

5. ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥
   â””â”€ ì¤‘ë³µ ì²´í¬ â†’ ë²Œí¬ ì‚½ì…
      â”œâ”€ crawl_notice í…Œì´ë¸”ì— ì €ì¥
      â””â”€ í†µê³„ ì—…ë°ì´íŠ¸

6. ì™„ë£Œ
   â””â”€ ì„±ê³µ/ì‹¤íŒ¨ ë¡œê¹…
      â””â”€ ë©”íŠ¸ë¦­ ìˆ˜ì§‘ (Prometheus)
```

### ìˆ˜ë™ ì‹¤í–‰ (API í˜¸ì¶œ)

```
1. POST /run-crawler/volunteer
   â””â”€ API í‚¤ ì¸ì¦

2. college_crawl_task.delay("ë´‰ì‚¬ ê³µì§€ì‚¬í•­ í¬ë¡¤ë§")
   â””â”€ ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì‹¤í–‰

3. ì‘ë‹µ ë°˜í™˜
   {
     "status": "success",
     "category": "volunteer",
     "message": "Crawling tasks triggered",
     "task_id": "abc-123-def"
   }

4. ë°±ê·¸ë¼ìš´ë“œ ì‹¤í–‰ (ìœ„ì™€ ë™ì¼)
```

---

## ğŸ“Š ëª¨ë‹ˆí„°ë§ ë° ë¡œê¹…

### Prometheus ë©”íŠ¸ë¦­

```python
# metrics.py
crawl_requests_total.inc()        # ì´ ìš”ì²­ ìˆ˜
crawl_errors_total.inc()          # ì—ëŸ¬ ìˆ˜
crawl_duration_seconds.observe()  # ì‹¤í–‰ ì‹œê°„
circuit_breaker_state.set(1)      # Circuit Breaker ìƒíƒœ
```

### ë¡œê¹…

```python
# logging_config.py
log_crawler_event("START", job_name, "PENDING")
log_crawler_event("COMPLETE", job_name, "SUCCESS")
log_performance("college_crawl_task", duration, {...})
log_error(exception, "crawl_volunteer", {...})
```

### Sentry ì—ëŸ¬ ì¶”ì 

```python
# sentry_config.py
track_crawler_error(
    exception=e,
    source="volunteer",
    url=url,
    page=page
)
```

---

## ğŸ¯ ì£¼ìš” ì„¤ê³„ ì›ì¹™

### 1. ì•ˆì •ì„± (Reliability)
- **Circuit Breaker**: ì—°ì† ì‹¤íŒ¨ ì‹œ ì‹œìŠ¤í…œ ë³´í˜¸
- **ì¬ì‹œë„ ë¡œì§**: ì§€ìˆ˜ ë°±ì˜¤í”„ë¡œ ì¼ì‹œì  ì—ëŸ¬ ì²˜ë¦¬
- **Celery**: ë¹„ë™ê¸° ì‘ì—… íë¡œ í™•ì¥ì„± ë³´ì¥

### 2. ì„±ëŠ¥ (Performance)
- **ë²Œí¬ ì‚½ì…**: ë°ì´í„°ë² ì´ìŠ¤ ì¿¼ë¦¬ ìµœì†Œí™”
- **ì¤‘ë³µ ì²´í¬ ìµœì í™”**: í•œ ë²ˆì— ì¡°íšŒ
- **Rate Limiting**: ì„œë²„ ë¶€í•˜ ë°©ì§€

### 3. ìœ ì§€ë³´ìˆ˜ì„± (Maintainability)
- **ëª¨ë“ˆí™”**: ê° ê¸°ëŠ¥ì„ ë…ë¦½ì ì¸ ëª¨ë“ˆë¡œ ë¶„ë¦¬
- **ë¡œê¹…**: ëª¨ë“  ì´ë²¤íŠ¸ ì¶”ì 
- **ë©”íŠ¸ë¦­**: Prometheusë¡œ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§

### 4. í™•ì¥ì„± (Scalability)
- **Celery Worker**: ìˆ˜í‰ í™•ì¥ ê°€ëŠ¥
- **Redis í**: ë¶„ì‚° ì²˜ë¦¬ ì§€ì›
- **Auto Scheduler**: ìƒˆ í¬ë¡¤ë§ ì‘ì—… ì‰½ê²Œ ì¶”ê°€

---

## ğŸš€ í¬ë¡¤ë§ ì‘ì—… ì¶”ê°€ ë°©ë²•

### 1. auto_scheduler.pyì— ì‘ì—… ì¶”ê°€

```python
job_configs = [
    # ê¸°ì¡´ ì‘ì—…ë“¤...
    {
        "name": "ìƒˆë¡œìš´ í¬ë¡¤ë§",
        "priority": "P2",
        "seed_payload": {
            "urls": ["https://www.inu.ac.kr/new/"],
            "category": "new_category",
            "page_num": "999"
        },
        "schedule_cron": "0 */3 * * *",  # 3ì‹œê°„ë§ˆë‹¤
        "rate_limit_per_host": 0.5,
        "max_depth": 1
    }
]
```

### 2. college_crawlers.pyì— í¬ë¡¤ëŸ¬ ë©”ì„œë“œ ì¶”ê°€

```python
def crawl_new_category(self) -> List[Dict[str, Any]]:
    """ìƒˆë¡œìš´ ì¹´í…Œê³ ë¦¬ í¬ë¡¤ë§"""
    # í¬ë¡¤ë§ ë¡œì§ êµ¬í˜„
    pass
```

### 3. tasks.pyì— ë¶„ê¸° ì¶”ê°€

```python
if job_name == "ìƒˆë¡œìš´ í¬ë¡¤ë§":
    results = college_crawler.crawl_new_category()
```

### 4. ì¬ì‹œì‘

```bash
# ìŠ¤ì¼€ì¤„ ê°•ì œ ì—…ë°ì´íŠ¸
POST /force-schedule-update

# ë˜ëŠ” ì„œë²„ ì¬ì‹œì‘
docker-compose restart crawling-server
```

---

ğŸ¤– Generated with [Claude Code](https://claude.com/claude-code)
