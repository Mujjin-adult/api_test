# tasks.py
import os
import time
import uuid
from celery import Celery
from datetime import timedelta, datetime
from celery.schedules import crontab
import hashlib
import logging

from config import get_redis_url, get_crawler_config
from database import get_db_context
from models import CrawlNotice
from crud import create_task, update_task_status, increment_task_retries, create_document, bulk_create_documents, get_job_by_name, get_document_by_url
from robots_parser import RobotsManager
from rate_limiter import get_rate_limiter
from url_utils import get_duplicate_checker, get_url_normalizer
from logging_config import log_crawler_event, log_error, log_performance
from college_crawlers import get_college_crawler
from slack_notify import send_slack_alert
import json

# ë¡œê±° ì„¤ì •
logger = logging.getLogger(__name__)

# Redis ì„¤ì •
BROKER_URL = get_redis_url()
RESULT_BACKEND = get_redis_url()

celery_app = Celery(
    "school_notices",
    broker=BROKER_URL,
    backend=RESULT_BACKEND,
)

# Celery ì„¤ì •
celery_app.conf.update(
    task_serializer="json",
    accept_content=["json"],
    result_serializer="json",
    timezone="Asia/Seoul",
    enable_utc=True,
    task_track_started=True,
    task_time_limit=30 * 60,  # 30ë¶„
    task_soft_time_limit=25 * 60,  # 25ë¶„
    worker_prefetch_multiplier=1,
    worker_max_tasks_per_child=1000,
)

# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤ë“¤
robots_manager = RobotsManager()
rate_limiter = get_rate_limiter()
duplicate_checker = get_duplicate_checker()
url_normalizer = get_url_normalizer()
college_crawler = get_college_crawler()


# ìš°ì„ ìˆœìœ„ í: priority (P0~P3)
PRIORITY_MAP = {"P0": 0, "P1": 1, "P2": 2, "P3": 3}


@celery_app.task(bind=True, max_retries=3, default_retry_delay=30)
def college_crawl_task(self, job_name: str):
    """ëŒ€í•™ ê³µì§€ì‚¬í•­ í¬ë¡¤ë§ íƒœìŠ¤í¬"""
    request_id = str(uuid.uuid4())
    start_time = time.time()

    try:
        logger.info(f"Starting college crawl task {request_id} for job: {job_name}")
        log_crawler_event("START", job_name, "PENDING", f"request_id={request_id}")

        # ëŒ€í•™ í¬ë¡¤ëŸ¬ë¡œ í¬ë¡¤ë§ ì‹¤í–‰
        if job_name == "ë´‰ì‚¬ ê³µì§€ì‚¬í•­ í¬ë¡¤ë§":
            results = college_crawler.crawl_volunteer()
        elif job_name == "ì·¨ì—… ê³µì§€ì‚¬í•­ í¬ë¡¤ë§":
            results = college_crawler.crawl_job()
        elif job_name == "ì¥í•™ê¸ˆ ê³µì§€ì‚¬í•­ í¬ë¡¤ë§":
            results = college_crawler.crawl_scholarship()
        else:
            # í†µí•© í¬ë¡¤ë§
            results = college_crawler.crawl_all()

        # ê²°ê³¼ ì²˜ë¦¬ ë° ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥
        total_items = 0
        saved_items = 0
        skipped_items = 0

        # ì¡ ID ì¡°íšŒ
        with get_db_context() as db:
            db_job = get_job_by_name(db, job_name)
            if not db_job:
                logger.warning(f"Job not found: {job_name}")
                job_id = None
            else:
                job_id = db_job.id

        # ê²°ê³¼ë¥¼ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥ (ë²Œí¬ ì‚½ì… ìµœì í™”)
        if isinstance(results, list):
            total_items = len(results)
            if job_id:
                # 1ë‹¨ê³„: ì¤‘ë³µ ì²´í¬ (í•œ ë²ˆì— ìˆ˜í–‰)
                docs_to_insert = []

                with get_db_context() as db:
                    # ëª¨ë“  URLì„ í•œ ë²ˆì— ì¡°íšŒ
                    urls_to_check = [item.get('url', '') for item in results if item.get('url')]
                    existing_urls = set()

                    if urls_to_check:
                        existing_docs = db.query(CrawlNotice).filter(CrawlNotice.url.in_(urls_to_check)).all()
                        existing_urls = {doc.url for doc in existing_docs}

                    # 2ë‹¨ê³„: ì‚½ì…í•  ë°ì´í„° ì¤€ë¹„
                    for item in results:
                        try:
                            item_url = item.get('url', '')

                            # ì¤‘ë³µ ì²´í¬
                            if item_url in existing_urls:
                                logger.debug(f"Document already exists: {item_url}")
                                skipped_items += 1
                                continue

                            # fingerprint ìƒì„±
                            fingerprint_str = f"{item_url}_{item.get('title', '')}"
                            fingerprint = hashlib.sha256(fingerprint_str.encode()).hexdigest()

                            # ë¬¸ì„œ ë°ì´í„° ìƒì„±
                            doc_data = {
                                "job_id": job_id,
                                "url": item_url,
                                "title": item.get('title'),
                                "writer": item.get('writer'),
                                "date": item.get('date'),
                                "hits": item.get('hits'),
                                "category": item.get('category'),
                                "source": item.get('source'),
                                "content": item.get('content', ''),  # ë³¸ë¬¸ ì¶”ê°€
                                "extracted": item,
                                "raw": json.dumps(item, ensure_ascii=False),
                                "fingerprint": fingerprint,
                                "snapshot_version": "v1"
                            }

                            docs_to_insert.append(doc_data)
                        except Exception as e:
                            logger.error(f"Failed to prepare document: {e}")
                            continue

                    # 3ë‹¨ê³„: ë²Œí¬ ì‚½ì…
                    if docs_to_insert:
                        try:
                            saved_count = bulk_create_documents(db, docs_to_insert)
                            saved_items = saved_count
                            logger.info(f"Bulk inserted {saved_count} documents")
                        except Exception as e:
                            logger.error(f"Bulk insert failed: {e}")
                            # í´ë°±: ê°œë³„ ì‚½ì…
                            for doc_data in docs_to_insert:
                                try:
                                    create_document(db, doc_data)
                                    saved_items += 1
                                except Exception as doc_error:
                                    logger.error(f"Failed to save document individually: {doc_error}")
                                    continue
        elif isinstance(results, dict):
            total_items = sum(len(items) for items in results.values())
            if job_id:
                # ëª¨ë“  ì¹´í…Œê³ ë¦¬ì˜ ì•„ì´í…œì„ í•˜ë‚˜ì˜ ë¦¬ìŠ¤íŠ¸ë¡œ í†µí•©
                all_items = []
                for category, items in results.items():
                    all_items.extend(items)

                # 1ë‹¨ê³„: ì¤‘ë³µ ì²´í¬ (í•œ ë²ˆì— ìˆ˜í–‰)
                docs_to_insert = []

                with get_db_context() as db:
                    # ëª¨ë“  URLì„ í•œ ë²ˆì— ì¡°íšŒ
                    urls_to_check = [item.get('url', '') for item in all_items if item.get('url')]
                    existing_urls = set()

                    if urls_to_check:
                        existing_docs = db.query(CrawlNotice).filter(CrawlNotice.url.in_(urls_to_check)).all()
                        existing_urls = {doc.url for doc in existing_docs}

                    # 2ë‹¨ê³„: ì‚½ì…í•  ë°ì´í„° ì¤€ë¹„
                    for item in all_items:
                        try:
                            item_url = item.get('url', '')

                            # ì¤‘ë³µ ì²´í¬
                            if item_url in existing_urls:
                                logger.debug(f"Document already exists: {item_url}")
                                skipped_items += 1
                                continue

                            # fingerprint ìƒì„±
                            fingerprint_str = f"{item_url}_{item.get('title', '')}"
                            fingerprint = hashlib.sha256(fingerprint_str.encode()).hexdigest()

                            # ë¬¸ì„œ ë°ì´í„° ìƒì„±
                            doc_data = {
                                "job_id": job_id,
                                "url": item_url,
                                "title": item.get('title'),
                                "writer": item.get('writer'),
                                "date": item.get('date'),
                                "hits": item.get('hits'),
                                "category": item.get('category'),
                                "source": item.get('source'),
                                "content": item.get('content', ''),  # ë³¸ë¬¸ ì¶”ê°€
                                "extracted": item,
                                "raw": json.dumps(item, ensure_ascii=False),
                                "fingerprint": fingerprint,
                                "snapshot_version": "v1"
                            }

                            docs_to_insert.append(doc_data)
                        except Exception as e:
                            logger.error(f"Failed to prepare document: {e}")
                            continue

                    # 3ë‹¨ê³„: ë²Œí¬ ì‚½ì…
                    if docs_to_insert:
                        try:
                            saved_count = bulk_create_documents(db, docs_to_insert)
                            saved_items = saved_count
                            logger.info(f"Bulk inserted {saved_count} documents")
                        except Exception as e:
                            logger.error(f"Bulk insert failed: {e}")
                            # í´ë°±: ê°œë³„ ì‚½ì…
                            for doc_data in docs_to_insert:
                                try:
                                    create_document(db, doc_data)
                                    saved_items += 1
                                except Exception as doc_error:
                                    logger.error(f"Failed to save document individually: {doc_error}")
                                    continue

        # ì„±ëŠ¥ ë¡œê¹…
        duration = time.time() - start_time
        log_performance(
            "college_crawl_task",
            duration,
            {"job_name": job_name, "total_items": total_items, "saved_items": saved_items, "skipped_items": skipped_items},
        )

        logger.info(
            f"College crawl task {request_id} completed successfully in {duration:.2f}s - Saved: {saved_items}, Skipped: {skipped_items}"
        )
        log_crawler_event(
            "COMPLETE",
            job_name,
            "SUCCESS",
            f"duration={duration:.2f}s, items={total_items}, saved={saved_items}, skipped={skipped_items}",
        )

        # âœ… Slack ì•Œë¦¼: í¬ë¡¤ë§ ì„±ê³µ
        slack_enabled = os.getenv("ENABLE_SLACK_NOTIFICATIONS", "false").lower() == "true"
        if slack_enabled:
            if saved_items > 0:
                message = (
                    f"âœ… *í¬ë¡¤ë§ ì„±ê³µ: {job_name}*\n\n"
                    f"â€¢ ì‹ ê·œ ë¬¸ì„œ: *{saved_items}ê°œ*\n"
                    f"â€¢ ì¤‘ë³µ ê±´ë„ˆëœ€: {skipped_items}ê°œ\n"
                    f"â€¢ ì´ ì²˜ë¦¬: {total_items}ê°œ\n"
                    f"â€¢ ì†Œìš” ì‹œê°„: {duration:.2f}ì´ˆ\n"
                    f"â€¢ ì™„ë£Œ ì‹œê°: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}"
                )
            else:
                message = (
                    f"â„¹ï¸ *í¬ë¡¤ë§ ì™„ë£Œ: {job_name}*\n\n"
                    f"â€¢ ì‹ ê·œ ë¬¸ì„œ: ì—†ìŒ\n"
                    f"â€¢ ì¤‘ë³µ ê±´ë„ˆëœ€: {skipped_items}ê°œ\n"
                    f"â€¢ ì†Œìš” ì‹œê°„: {duration:.2f}ì´ˆ\n"
                    f"â€¢ ì™„ë£Œ ì‹œê°: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}"
                )

            try:
                send_slack_alert(message)
            except Exception as slack_error:
                logger.warning(f"Failed to send Slack notification: {slack_error}")

        return {
            "status": "success",
            "job_name": job_name,
            "total_items": total_items,
            "saved_items": saved_items,
            "skipped_items": skipped_items,
            "duration": duration,
        }

    except Exception as exc:
        duration = time.time() - start_time
        error_msg = str(exc)

        logger.error(f"College crawl task {request_id} failed: {error_msg}")
        log_error(
            exc,
            f"college_crawl_task for {job_name}",
            {
                "job_name": job_name,
                "duration": duration,
            },
        )

        # âŒ Slack ì•Œë¦¼: í¬ë¡¤ë§ ì—ëŸ¬
        slack_enabled = os.getenv("ENABLE_SLACK_NOTIFICATIONS", "false").lower() == "true"
        if slack_enabled:
            retry_count = self.request.retries
            max_retries = self.max_retries

            message = (
                f"âŒ *í¬ë¡¤ë§ ì‹¤íŒ¨: {job_name}*\n\n"
                f"â€¢ ì—ëŸ¬: `{error_msg}`\n"
                f"â€¢ ì¬ì‹œë„: {retry_count}/{max_retries}\n"
                f"â€¢ ì†Œìš” ì‹œê°„: {duration:.2f}ì´ˆ\n"
                f"â€¢ ë°œìƒ ì‹œê°: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n"
                f"{'âš ï¸ ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ë„ë‹¬!' if retry_count >= max_retries else 'ğŸ”„ ìë™ìœ¼ë¡œ ì¬ì‹œë„í•©ë‹ˆë‹¤...'}"
            )

            try:
                send_slack_alert(message)
            except Exception as slack_error:
                logger.warning(f"Failed to send Slack notification: {slack_error}")

        # ì¬ì‹œë„ ë¡œì§
        try:
            self.retry(exc=exc, countdown=(2**self.request.retries) + 5)
        except self.MaxRetriesExceededError:
            log_crawler_event("MAX_RETRIES", job_name, "FAILED", f"error={error_msg}")

            # ìµœëŒ€ ì¬ì‹œë„ ì‹¤íŒ¨ ì‹œ Slack ì•Œë¦¼
            if slack_enabled:
                final_message = (
                    f"ğŸš¨ *í¬ë¡¤ë§ ìµœì¢… ì‹¤íŒ¨: {job_name}*\n\n"
                    f"â€¢ ì—ëŸ¬: `{error_msg}`\n"
                    f"â€¢ ëª¨ë“  ì¬ì‹œë„ ì‹¤íŒ¨\n"
                    f"â€¢ ë°œìƒ ì‹œê°: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n"
                    f"âš ï¸ ê´€ë¦¬ì í™•ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤!"
                )
                try:
                    send_slack_alert(final_message)
                except:
                    pass

            return {"status": "failed", "error": error_msg, "job_name": job_name}


@celery_app.task(bind=True, max_retries=3, default_retry_delay=30)
def crawl_task(
    self, job_id: int, url: str, priority: str = "P2", render_mode: str = "STATIC"
):
    """í¬ë¡¤ë§ íƒœìŠ¤í¬ ì‹¤í–‰"""
    request_id = str(uuid.uuid4())
    start_time = time.time()

    try:
        logger.info(f"Starting crawl task {request_id} for URL: {url}")
        log_crawler_event(
            "START", url, "PENDING", f"job_id={job_id}, priority={priority}"
        )

        # 1. ì¤‘ë³µ ì²´í¬
        if duplicate_checker.is_duplicate(url):
            logger.info(f"URL already crawled: {url}")
            log_crawler_event("DUPLICATE", url, "SKIPPED", f"job_id={job_id}")
            return {"status": "skipped", "reason": "duplicate", "url": url}

        # 2. robots.txt ì¤€ìˆ˜ í™•ì¸
        crawler_config = get_crawler_config()
        if crawler_config.get("respect_robots_txt", True):
            if not robots_manager.is_allowed(url, crawler_config.get("user_agent")):
                logger.warning(f"URL blocked by robots.txt: {url}")
                log_crawler_event("ROBOTS_BLOCKED", url, "BLOCKED", f"job_id={job_id}")
                return {"status": "blocked", "reason": "robots_txt", "url": url}

            # crawl-delay ì¤€ìˆ˜
            robots_manager.wait_if_needed(url, crawler_config.get("user_agent"))

        # 3. ë ˆì´íŠ¸ ë¦¬ë¯¸íŒ… í™•ì¸
        if not rate_limiter.can_make_request(url):
            logger.info(f"Rate limit reached for host, waiting...")
            if not rate_limiter.wait_for_request(url, timeout=60):
                raise Exception("Rate limit timeout exceeded")

        # 4. ë°ì´í„°ë² ì´ìŠ¤ì— íƒœìŠ¤í¬ ìƒíƒœ ì—…ë°ì´íŠ¸
        with get_db_context() as db:
            db_task = create_task(
                db, {"job_id": job_id, "url": url, "status": "RUNNING"}
            )
            task_id = db_task.id

        # 5. ì‹¤ì œ í¬ë¡¤ë§ ì‹¤í–‰
        result = execute_crawling(url, render_mode, request_id)

        # 6. ê²°ê³¼ ì €ì¥ ë° ìƒíƒœ ì—…ë°ì´íŠ¸
        with get_db_context() as db:
            update_task_status(
                db,
                task_id,
                "SUCCESS",
                http_status=result.get("http_status"),
                content_hash=result.get("content_hash"),
            )

        # 7. ì¤‘ë³µ ì²´í¬ì— URL ì¶”ê°€
        duplicate_checker.mark_as_seen(url)

        # 8. ì„±ëŠ¥ ë¡œê¹…
        duration = time.time() - start_time
        log_performance(
            "crawl_task",
            duration,
            {"url": url, "render_mode": render_mode, "priority": priority},
        )

        logger.info(
            f"Crawl task {request_id} completed successfully in {duration:.2f}s"
        )
        log_crawler_event("COMPLETE", url, "SUCCESS", f"duration={duration:.2f}s")

        return result

    except Exception as exc:
        duration = time.time() - start_time
        error_msg = str(exc)

        logger.error(f"Crawl task {request_id} failed: {error_msg}")
        log_error(
            exc,
            f"crawl_task for {url}",
            {
                "job_id": job_id,
                "priority": priority,
                "render_mode": render_mode,
                "duration": duration,
            },
        )

        # ë°ì´í„°ë² ì´ìŠ¤ ìƒíƒœ ì—…ë°ì´íŠ¸
        try:
            with get_db_context() as db:
                if "task_id" in locals():
                    update_task_status(db, task_id, "FAILED", error=error_msg)
        except Exception as db_error:
            logger.error(f"Failed to update task status in database: {db_error}")

        # ì¬ì‹œë„ ë¡œì§
        try:
            self.retry(exc=exc, countdown=(2**self.request.retries) + 5)
        except self.MaxRetriesExceededError:
            log_crawler_event("MAX_RETRIES", url, "FAILED", f"error={error_msg}")
            return {"status": "failed", "error": error_msg, "url": url}


def execute_crawling(url: str, render_mode: str, request_id: str) -> dict:
    """ì‹¤ì œ í¬ë¡¤ë§ ì‹¤í–‰"""
    start_time = time.time()

    try:
        if render_mode == "STATIC":
            result = crawl_static(url, request_id)
        elif render_mode == "HEADLESS":
            result = crawl_headless(url, request_id)
        elif render_mode == "AUTO":
            # ì •ì  ë¨¼ì € ì‹œë„, ì‹¤íŒ¨ ì‹œ í—¤ë“œë¦¬ìŠ¤ë¡œ ì „í™˜
            try:
                result = crawl_static(url, request_id)
            except Exception as static_error:
                logger.info(f"Static crawling failed, trying headless: {static_error}")
                result = crawl_headless(url, request_id)
        else:
            raise ValueError(f"Unknown render mode: {render_mode}")

        # ë¸Œë¼ìš°ì € ì‚¬ìš© ì‹œê°„ ê¸°ë¡
        browser_time = int((time.time() - start_time) * 1000)  # ë°€ë¦¬ì´ˆ
        result["cost_ms_browser"] = browser_time

        # ë ˆì´íŠ¸ ë¦¬ë¯¸í„°ì— ìš”ì²­ ì™„ë£Œ ê¸°ë¡
        rate_limiter.record_request_end(url, request_id, browser_time)

        return result

    except Exception as e:
        rate_limiter.record_request_end(url, request_id)
        raise e


def crawl_static(url: str, request_id: str) -> dict:
    """ì •ì  í¬ë¡¤ë§ (requests ì‚¬ìš©)"""
    import requests
    from config import get_crawler_config

    config = get_crawler_config()
    headers = {"User-Agent": config.get("user_agent")}

    response = requests.get(
        url,
        headers=headers,
        timeout=config.get("request_timeout", 30),
        stream=True,  # ëŒ€ìš©ëŸ‰ íŒŒì¼ ì²˜ë¦¬
    )

    # ì‘ë‹µ í¬ê¸° í™•ì¸
    content_length = response.headers.get("content-length")
    if content_length and int(content_length) > config.get(
        "max_content_length", 10 * 1024 * 1024
    ):
        raise ValueError(f"Content too large: {content_length} bytes")

    response.raise_for_status()

    # ë‚´ìš© ì½ê¸°
    content = response.content.decode("utf-8", errors="ignore")

    # ê°„ë‹¨í•œ íŒŒì‹± (ì˜ˆì‹œ)
    extracted_data = parse_static_content(content, url)

    return {
        "url": url,
        "status": "success",
        "http_status": response.status_code,
        "content_hash": hashlib.sha256(content.encode()).hexdigest(),
        "data": extracted_data,
        "render_mode": "STATIC",
    }


def crawl_headless(url: str, request_id: str) -> dict:
    """í—¤ë“œë¦¬ìŠ¤ ë¸Œë¼ìš°ì € í¬ë¡¤ë§ (Playwright ì‚¬ìš©)"""
    from playwright_crawler import crawl_with_headless
    from config import get_playwright_config

    config = get_playwright_config()

    # Playwrightë¡œ í¬ë¡¤ë§
    html_content = crawl_with_headless(url, config)

    # ê°„ë‹¨í•œ íŒŒì‹± (ì˜ˆì‹œ)
    extracted_data = parse_static_content(html_content, url)

    return {
        "url": url,
        "status": "success",
        "http_status": 200,
        "content_hash": hashlib.sha256(html_content.encode()).hexdigest(),
        "data": extracted_data,
        "render_mode": "HEADLESS",
    }


def parse_static_content(content: str, url: str) -> dict:
    """ì •ì  ì½˜í…ì¸  íŒŒì‹± (ê¸°ë³¸ êµ¬í˜„)"""
    # TODO: ì‹¤ì œ íŒŒì‹± ë¡œì§ êµ¬í˜„
    # í˜„ì¬ëŠ” ê¸°ë³¸ ì •ë³´ë§Œ ì¶”ì¶œ

    import re

    # ì œëª© ì¶”ì¶œ (ê°„ë‹¨í•œ ì •ê·œì‹)
    title_match = re.search(r"<title[^>]*>([^<]+)</title>", content, re.IGNORECASE)
    title = title_match.group(1).strip() if title_match else "No title"

    # ë§í¬ ì¶”ì¶œ
    links = re.findall(r'href=["\']([^"\']+)["\']', content)

    # í…ìŠ¤íŠ¸ ê¸¸ì´
    text_content = re.sub(r"<[^>]+>", "", content)
    text_length = len(text_content.strip())

    return {
        "title": title,
        "links_count": len(links),
        "text_length": text_length,
        "url": url,
    }


@celery_app.task
def send_daily_report():
    """ì¼ì¼ ìš”ì•½ ë¦¬í¬íŠ¸ ì „ì†¡ (ë§¤ì¼ ì˜¤ì „ 9ì‹œ)"""
    try:
        logger.info("Generating daily report...")

        # ì–´ì œ ë‚ ì§œ ê³„ì‚°
        from sqlalchemy import func, and_
        yesterday = datetime.now() - timedelta(days=1)
        yesterday_start = yesterday.replace(hour=0, minute=0, second=0, microsecond=0)
        yesterday_end = yesterday.replace(hour=23, minute=59, second=59, microsecond=999999)

        with get_db_context() as db:
            # ì–´ì œ ìˆ˜ì§‘ëœ ë¬¸ì„œ í†µê³„
            total_docs = db.query(func.count(CrawlNotice.id)).filter(
                and_(
                    CrawlNotice.created_at >= yesterday_start,
                    CrawlNotice.created_at <= yesterday_end
                )
            ).scalar() or 0

            # ì¹´í…Œê³ ë¦¬ë³„ í†µê³„
            from sqlalchemy import distinct
            category_stats = db.query(
                CrawlNotice.category,
                func.count(CrawlNotice.id).label('count')
            ).filter(
                and_(
                    CrawlNotice.created_at >= yesterday_start,
                    CrawlNotice.created_at <= yesterday_end
                )
            ).group_by(CrawlNotice.category).all()

            # ì „ì²´ ë¬¸ì„œ ìˆ˜
            total_in_db = db.query(func.count(CrawlNotice.id)).scalar() or 0

        # Slack ë©”ì‹œì§€ ìƒì„±
        slack_enabled = os.getenv("ENABLE_SLACK_NOTIFICATIONS", "false").lower() == "true"
        if slack_enabled:
            # ì¹´í…Œê³ ë¦¬ë³„ í†µê³„ í¬ë§·íŒ…
            category_details = ""
            if category_stats:
                for category, count in category_stats:
                    category_details += f"  - {category or 'ë¯¸ë¶„ë¥˜'}: {count}ê°œ\n"
            else:
                category_details = "  - ìˆ˜ì§‘ëœ ë¬¸ì„œ ì—†ìŒ\n"

            message = (
                f"ğŸ“Š *ì¼ì¼ í¬ë¡¤ë§ ë¦¬í¬íŠ¸*\n"
                f"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n"
                f"ğŸ“… ë‚ ì§œ: {yesterday.strftime('%Yë…„ %mì›” %dì¼')}\n\n"
                f"âœ¨ *ì–´ì œ ìˆ˜ì§‘ í˜„í™©*\n"
                f"â€¢ ì‹ ê·œ ë¬¸ì„œ: *{total_docs}ê°œ*\n\n"
                f"ğŸ“‚ *ì¹´í…Œê³ ë¦¬ë³„ ìˆ˜ì§‘ í˜„í™©*\n"
                f"{category_details}\n"
                f"ğŸ’¾ *ì „ì²´ ë°ì´í„°ë² ì´ìŠ¤*\n"
                f"â€¢ ì´ ë¬¸ì„œ ìˆ˜: {total_in_db:,}ê°œ\n\n"
                f"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n"
                f"ğŸ• ë¦¬í¬íŠ¸ ìƒì„± ì‹œê°: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}"
            )

            send_slack_alert(message)
            logger.info(f"Daily report sent successfully: {total_docs} docs collected yesterday")

        return {
            "status": "success",
            "total_docs_yesterday": total_docs,
            "total_docs_in_db": total_in_db,
            "category_stats": dict(category_stats) if category_stats else {}
        }

    except Exception as e:
        logger.error(f"Failed to generate daily report: {e}")

        # ì—ëŸ¬ ë°œìƒ ì‹œì—ë„ Slack ì•Œë¦¼
        slack_enabled = os.getenv("ENABLE_SLACK_NOTIFICATIONS", "false").lower() == "true"
        if slack_enabled:
            error_message = (
                f"âŒ *ì¼ì¼ ë¦¬í¬íŠ¸ ìƒì„± ì‹¤íŒ¨*\n\n"
                f"â€¢ ì—ëŸ¬: `{str(e)}`\n"
                f"â€¢ ë°œìƒ ì‹œê°: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}"
            )
            try:
                send_slack_alert(error_message)
            except:
                pass

        return {"status": "failed", "error": str(e)}


# Beat ìŠ¤ì¼€ì¤„ëŸ¬ ë“±ë¡
celery_app.conf.beat_schedule = {
    # ==================== ëŒ€í•™ ê³µì§€ì‚¬í•­ í¬ë¡¤ë§ ìŠ¤ì¼€ì¤„ ====================
    # ë´‰ì‚¬ ê³µì§€ì‚¬í•­ (2ì‹œê°„ë§ˆë‹¤)
    "college-ë´‰ì‚¬-ê³µì§€ì‚¬í•­-í¬ë¡¤ë§": {
        "task": "tasks.college_crawl_task",
        "schedule": crontab(minute=0, hour="*/2"),
        "args": ("ë´‰ì‚¬ ê³µì§€ì‚¬í•­ í¬ë¡¤ë§",),
        "options": {"priority": PRIORITY_MAP["P1"]},
    },
    # ì·¨ì—… ê³µì§€ì‚¬í•­ (3ì‹œê°„ë§ˆë‹¤)
    "college-ì·¨ì—…-ê³µì§€ì‚¬í•­-í¬ë¡¤ë§": {
        "task": "tasks.college_crawl_task",
        "schedule": crontab(minute=0, hour="*/3"),
        "args": ("ì·¨ì—… ê³µì§€ì‚¬í•­ í¬ë¡¤ë§",),
        "options": {"priority": PRIORITY_MAP["P1"]},
    },
    # ì¥í•™ê¸ˆ ê³µì§€ì‚¬í•­ (4ì‹œê°„ë§ˆë‹¤)
    "college-ì¥í•™ê¸ˆ-ê³µì§€ì‚¬í•­-í¬ë¡¤ë§": {
        "task": "tasks.college_crawl_task",
        "schedule": crontab(minute=0, hour="*/4"),
        "args": ("ì¥í•™ê¸ˆ ê³µì§€ì‚¬í•­ í¬ë¡¤ë§",),
        "options": {"priority": PRIORITY_MAP["P1"]},
    },
    # ì¼ë°˜í–‰ì‚¬/ì±„ìš© (6ì‹œê°„ë§ˆë‹¤)
    "college-ì¼ë°˜í–‰ì‚¬-ì±„ìš©-í¬ë¡¤ë§": {
        "task": "tasks.college_crawl_task",
        "schedule": crontab(minute=0, hour="*/6"),
        "args": ("ì¼ë°˜í–‰ì‚¬/ì±„ìš© í¬ë¡¤ë§",),
        "options": {"priority": PRIORITY_MAP["P2"]},
    },
    # êµìœ¡ì‹œí—˜ (6ì‹œê°„ë§ˆë‹¤)
    "college-êµìœ¡ì‹œí—˜-í¬ë¡¤ë§": {
        "task": "tasks.college_crawl_task",
        "schedule": crontab(minute=0, hour="*/6"),
        "args": ("êµìœ¡ì‹œí—˜ í¬ë¡¤ë§",),
        "options": {"priority": PRIORITY_MAP["P2"]},
    },
    # ë“±ë¡ê¸ˆë‚©ë¶€ (8ì‹œê°„ë§ˆë‹¤)
    "college-ë“±ë¡ê¸ˆë‚©ë¶€-í¬ë¡¤ë§": {
        "task": "tasks.college_crawl_task",
        "schedule": crontab(minute=0, hour="*/8"),
        "args": ("ë“±ë¡ê¸ˆë‚©ë¶€ í¬ë¡¤ë§",),
        "options": {"priority": PRIORITY_MAP["P2"]},
    },
    # í•™ì  (8ì‹œê°„ë§ˆë‹¤)
    "college-í•™ì -í¬ë¡¤ë§": {
        "task": "tasks.college_crawl_task",
        "schedule": crontab(minute=0, hour="*/8"),
        "args": ("í•™ì  í¬ë¡¤ë§",),
        "options": {"priority": PRIORITY_MAP["P2"]},
    },
    # í•™ìœ„ (8ì‹œê°„ë§ˆë‹¤)
    "college-í•™ìœ„-í¬ë¡¤ë§": {
        "task": "tasks.college_crawl_task",
        "schedule": crontab(minute=0, hour="*/8"),
        "args": ("í•™ìœ„ í¬ë¡¤ë§",),
        "options": {"priority": PRIORITY_MAP["P2"]},
    },
    # ==================== ì‹œìŠ¤í…œ íƒœìŠ¤í¬ ====================
    # ì¼ì¼ ìš”ì•½ ë¦¬í¬íŠ¸ (ë§¤ì¼ ì˜¤ì „ 9ì‹œ)
    "daily-report": {
        "task": "tasks.send_daily_report",
        "schedule": crontab(hour=9, minute=0),
    },
}

# ë¦¬í”„ë ˆì‹œ/ë°ë“œë ˆí„° í/ìš°ì„ ìˆœìœ„ í ë“±ì€ celery ì„¤ì •ì—ì„œ routing/queueë¡œ í™•ì¥ ê°€ëŠ¥
